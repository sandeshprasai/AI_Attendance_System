{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cc4436-7fad-41ca-b12a-c5572b3d48f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training detection model with YOLOv8...\n",
      "Ultralytics 8.3.233  Python-3.8.20 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=D:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\dataset.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=120, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=False, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=D:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 39.98.4 MB/s, size: 15.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\labels\\train.cache... 3023 images, 27 backgrounds, 109 corrupt: 100% ━━━━━━━━━━━━ 3043/3043  0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Aishwarya_Rai_train_000038.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0912      1.1183]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Aishwarya_Rai_val_000048.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1177]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Akshay_Kumar_train_000056.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0809]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Akshay_Kumar_train_000082.jpg: ignoring corrupt image/label: negative class labels or coordinate [  -0.025083]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Akshay_Kumar_train_000095.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.051]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Akshay_Kumar_train_000097.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0555]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Akshay_Kumar_train_Bollywood actor Akshay Kumar at the press conference of his upcoming film 'Namastey London' at Le-Meriden hotel, New Delhi, on 19th March '07..jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0697]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Alia_Bhatt_test_000017.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2641      1.3043]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Alia_Bhatt_test_000048.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1096      1.2002]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Alia_Bhatt_train_000005.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0182      1.1336]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Allu_Arjun_train_000050.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.208]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Amitabh_Bachchan_train_000064.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.249]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Ariana_Grande_train_000040.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0952       1.232]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Ariana_Grande_train_Ariana Grande attends the 2025 MTV Video Music Awards at UBS Arena on September 07, 2025 in Elmont, New York.-008.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0649      1.0814]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Barack_Obama_train_000001.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2329      1.7987]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Bill_Gates_train_000056.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0471]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Brad_Pitt_train_000083.jpg: ignoring corrupt image/label: negative class labels or coordinate [  -0.056343]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Chris_Evans_train_000092.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2851       1.734]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Cristiano_Ronaldo_test_000077.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0575      1.4133]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Cristiano_Ronaldo_train_000104.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3068]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Deepika_Padukone_test_000043.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0371      1.1082]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Deepika_Padukone_train_000008.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0539]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Deepika_Padukone_train_000012.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3437]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Deepika_Padukone_train_000017.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0514]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Deepika_Padukone_train_000036.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0387]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Drake_val_000003.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0145       1.109]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Dwayne_Johnson_train_000035.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.102]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Ed_Sheeran_test_000058.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2358]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_test_000046.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1527]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_test_000056.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0839]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000040.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0331]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000043.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0974]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000048.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4423      1.6822]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000050.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0319]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000051.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.137]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000054.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0378]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000059.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0509      1.1577]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000063.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1398]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000065.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2773      1.4448]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000074.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1252      1.3576]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000077.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0558      1.2546]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000081.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.4065      1.6396]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_train_000082.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0757]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Emma_Watson_val_000053.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2316]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Hrithik_Roshan_train_Actor Hrithik Roshan, star of the film Kites attends a photcall at the Majestic Hotel, in Cannes, France, as part of the 62nd annual Cannes Film....jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3183]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Hrithik_Roshan_val_000024.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0586]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Jeff_Bezos_train_000099.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0589]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Justin_Bieber_train_000073.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3075      1.6258]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Justin_Bieber_train_000093.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1081]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Justin_Bieber_train_000095.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1398      1.4533]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kareena_Kapoor_train_000050.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0485]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kim_Kardashian_train_000014.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3271      1.6625]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_test_000041.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1475]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_test_000067.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1482]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000002.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3816      1.7418]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000008.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3188      1.5516]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000014.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0601]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000017.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1929       1.247]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000020.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0623      1.1636]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000021.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0365       1.134]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000024.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1373      1.4202]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000026.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0926]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000027.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0848      1.2265]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000031.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1686       1.367]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_train_000033.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2997      1.5379]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_val_000009.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.141      1.2696]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Kylie_Jenner_val_000028.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.591      1.7179]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Leonardo_DiCaprio_train_000029.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.412]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Leonardo_DiCaprio_train_000090.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1412      1.2795]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Mark_Zuckerberg_train_Facebook co-founder and CEO Mark Zuckerberg arrives to testify before the House Financial Services Committee in the Rayburn House Office Building on...-001.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0475      1.5261]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Mary_Kom_test_000032.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1898]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Mary_Kom_train_000008.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1032]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Mary_Kom_train_000009.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3077]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Mary_Kom_train_000014.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0355       1.246      1.3304]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Michael_Jordan_test_000050.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0913]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Michael_Jordan_test_000105.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2693]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Michael_Jordan_train_000078.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1378]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Ranveer_Singh_val_000054.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0982]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Rihanna_train_000017.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0448]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Robert_Downey_Jr_train_000017.jpg: ignoring corrupt image/label: negative class labels or coordinate [  -0.044612]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Robert_Downey_Jr_train_000058.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0365]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Sachin_Tendulkar_train_000068.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1389]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Salman_Khan_val_Actor Salman Khan at a function in Noida on Friday.-003.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0437      1.1004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Scarlett_Johansson_train_000039.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1208]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000087.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0616]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000092.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3332      1.5804]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000094.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0164      1.1544]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000095.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0906]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000097.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1515]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000098.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0614      1.1491]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000101.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1591      1.3007]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000103.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2395      1.6109]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000110.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0727      1.3486]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000111.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2903      1.5762]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_train_000112.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0638]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Selena_Gomez_val_000102.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0437]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Tom_Holland_test_000096.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2848      1.3636]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Virat_Kohli_train_000004.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2689      1.4411]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Virat_Kohli_train_000025.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1709]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Virat_Kohli_train_000081.jpg: ignoring corrupt image/label: negative class labels or coordinate [  -0.011519]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Virat_Kohli_val_000050.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0435]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Will_Smith_train_000054.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.053]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Will_Smith_val_000082.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1889]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_test_000002.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0289]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_train_000027.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0352]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_train_000055.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0233]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_train_000066.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1357      1.2918]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_train_000097.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0557]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\train\\Zendaya_train_000098.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2546      1.5363]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.1 ms, read: 23.75.1 MB/s, size: 14.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\labels\\val.cache... 756 images, 1 backgrounds, 26 corrupt: 100% ━━━━━━━━━━━━ 756/756 509.4Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Aishwarya_Rai_train_000066.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1184      1.6361]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Alia_Bhatt_train_000020.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2683]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Alia_Bhatt_train_000025.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1431]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Amitabh_Bachchan_train_Amitabh Bachchan arrives for the International Indian Film Academy awards at Hallam Arena in Sheffield..jpg: ignoring corrupt image/label: negative class labels or coordinate [  -0.046564]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Chris_Evans_train_000026.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1475]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Deepika_Padukone_train_000010.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1957      1.2466]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Emma_Watson_test_000039.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0281      1.2214]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Emma_Watson_train_000072.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3102       1.529]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Jeff_Bezos_val_000021.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0571]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Justin_Bieber_train_000075.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.6605      2.1312]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kamal_Haasan_train_000093.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0168]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kareena_Kapoor_train_000025.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3546]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kareena_Kapoor_val_Siddharth Roy Kapur, Bollywood film producer and the CEO of UTV Motion Pictures during the theatrical trailer release of film Satyagraha at Taj Lands...-001.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0491]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kim_Kardashian_val_000028.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0452]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kylie_Jenner_train_000019.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2997      1.5379]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kylie_Jenner_train_000023.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1038      1.3223]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kylie_Jenner_train_000044.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2681]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Kylie_Jenner_train_000071.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1032      1.4458]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Leonardo_DiCaprio_train_000087.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0473]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Mary_Kom_train_000030.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2197]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Michael_Jordan_train_000004.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0307]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Selena_Gomez_train_000018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0107]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Selena_Gomez_train_000093.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0193]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Selena_Gomez_train_000107.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0844      1.4151]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Tom_Holland_train_000099.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.1596      1.1921]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mD:\\Final_Semester_Project\\AI_Attendance_System\\AI_And_ML_Model\\DataSets\\Detection\\images\\val\\Zendaya_val_000021.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.3305       1.635]\n",
      "Plotting labels to D:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mD:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\u001b[0m\n",
      "Starting training for 120 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/120     0.324G      2.308      1.975      2.574         18        224: 100% ━━━━━━━━━━━━ 184/184 8.4it/s 22.0s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.0it/s 3.3s0.1s\n",
      "                   all        730        736      0.909      0.914      0.957      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/120     0.324G        1.5      1.138       1.89         15        224: 100% ━━━━━━━━━━━━ 184/184 10.0it/s 18.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.8it/s 3.4s0.1s\n",
      "                   all        730        736       0.95      0.918      0.967      0.638\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/120     0.324G      1.321     0.9574       1.71          9        224: 100% ━━━━━━━━━━━━ 184/184 10.2it/s 18.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.7it/s 3.4s0.1s\n",
      "                   all        730        736      0.969      0.982      0.993      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/120     0.324G      1.225     0.8521      1.617         11        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.0s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.4it/s 3.1s0.1s\n",
      "                   all        730        736      0.991      0.989      0.995      0.766\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/120     0.324G      1.189     0.8132      1.583         18        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.6it/s 3.5s0.2s\n",
      "                   all        730        736      0.987      0.978       0.99      0.718\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/120     0.324G      1.135      0.748      1.549         13        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.4it/s 3.1s0.1s\n",
      "                   all        730        736      0.992      0.981      0.994      0.761\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/120     0.324G      1.091     0.7007      1.517         16        224: 100% ━━━━━━━━━━━━ 184/184 10.1it/s 18.1s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.4it/s 3.1s0.1s\n",
      "                   all        730        736      0.996      0.986      0.995      0.747\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/120     0.324G      1.062     0.6803      1.494         16        224: 100% ━━━━━━━━━━━━ 184/184 10.5it/s 17.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.6it/s 3.5s0.1s\n",
      "                   all        730        736       0.99      0.989      0.995      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/120     0.324G      1.029     0.6411      1.468         12        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.0it/s 3.3s0.1s\n",
      "                   all        730        736      0.996      0.988      0.995      0.763\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/120     0.326G      1.004     0.6235      1.447         15        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.3s.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.988      0.988      0.771\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/120     0.326G     0.9961     0.6039      1.441         17        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.0s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.993       0.99      0.995      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/120     0.326G     0.9756     0.5978      1.419         11        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.3s0.1s\n",
      "                   all        730        736      0.996      0.988      0.994      0.806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/120     0.326G     0.9605     0.5855      1.412         15        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.8s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.2s0.1s\n",
      "                   all        730        736      0.995       0.99      0.995      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/120     0.326G      0.954     0.5734      1.406         12        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.995      0.986      0.986      0.799\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/120     0.326G     0.9272     0.5561      1.383         12        224: 100% ━━━━━━━━━━━━ 184/184 11.1it/s 16.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.989      0.995      0.791\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/120     0.326G     0.9254     0.5477      1.378         19        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.989      0.995      0.776\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/120     0.326G     0.9195       0.55       1.37         18        224: 100% ━━━━━━━━━━━━ 184/184 10.5it/s 17.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.2s\n",
      "                   all        730        736      0.997       0.99      0.995      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/120     0.326G     0.9145     0.5395      1.375         14        224: 100% ━━━━━━━━━━━━ 184/184 10.4it/s 17.6s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.7it/s 3.4s0.1s\n",
      "                   all        730        736      0.999       0.99      0.995      0.793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/120     0.326G     0.9171     0.5462      1.373          8        224: 100% ━━━━━━━━━━━━ 184/184 10.2it/s 18.0s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.8it/s 3.4s0.1s\n",
      "                   all        730        736      0.993      0.991      0.995      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/120     0.326G     0.8963     0.5398      1.357         16        224: 100% ━━━━━━━━━━━━ 184/184 10.3it/s 17.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.989      0.989      0.995      0.797\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/120     0.326G     0.8866     0.5258      1.353         14        224: 100% ━━━━━━━━━━━━ 184/184 10.1it/s 18.1s.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.996      0.989      0.995      0.814\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/120     0.326G     0.8838     0.5177      1.356         14        224: 100% ━━━━━━━━━━━━ 184/184 10.3it/s 17.9s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.9it/s 3.3s0.1s\n",
      "                   all        730        736      0.997      0.989      0.995       0.81\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/120     0.326G     0.8853     0.5278      1.349         12        224: 100% ━━━━━━━━━━━━ 184/184 9.9it/s 18.6s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.997       0.99      0.995      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/120     0.326G     0.8673     0.5136      1.336         17        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/120     0.326G     0.8714     0.5016       1.34         17        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.8s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.5it/s 3.1s0.1s\n",
      "                   all        730        736      0.994       0.99      0.995      0.801\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/120     0.326G     0.8804     0.4999      1.346         10        224: 100% ━━━━━━━━━━━━ 184/184 9.7it/s 19.1s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.7it/s 3.4s0.2s\n",
      "                   all        730        736      0.997       0.99      0.995      0.812\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/120     0.326G     0.8691     0.5043      1.341         10        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.0it/s 3.3s0.1s\n",
      "                   all        730        736      0.999      0.989      0.995      0.805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/120     0.326G      0.864     0.4958       1.33         19        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.3s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.996      0.992      0.995      0.817\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/120     0.326G     0.8349     0.4815      1.312         11        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.5it/s 3.6s0.1s\n",
      "                   all        730        736      0.999       0.99      0.995      0.821\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/120     0.326G     0.8502     0.4879      1.329         11        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.988      0.995      0.824\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/120     0.326G     0.8554     0.4867      1.323          8        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.992       0.99      0.995      0.822\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/120     0.326G     0.8281     0.4806      1.305         13        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.992      0.995      0.824\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/120     0.326G     0.8342     0.4849      1.303         15        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.1s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.809\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/120     0.326G     0.8184     0.4741      1.294         10        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.2s\n",
      "                   all        730        736      0.999      0.989      0.995      0.828\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/120     0.326G      0.829     0.4727      1.308         13        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.1s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.811\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/120     0.326G      0.828     0.4784      1.298         13        224: 100% ━━━━━━━━━━━━ 184/184 9.9it/s 18.5s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.7it/s 3.4s0.1s\n",
      "                   all        730        736      0.999       0.99      0.995      0.821\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/120     0.326G     0.8251     0.4687      1.299         18        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.2s0.1s\n",
      "                   all        730        736      0.996       0.99      0.995      0.823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/120     0.326G     0.8153     0.4648      1.293         14        224: 100% ━━━━━━━━━━━━ 184/184 9.9it/s 18.6s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.997      0.992      0.995      0.828\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/120     0.326G     0.8114     0.4563      1.293         19        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/120     0.326G     0.8126     0.4609      1.289         16        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.1s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.822\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/120     0.326G     0.8021     0.4497      1.284         14        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.1s0.1s\n",
      "                   all        730        736      0.997      0.992      0.995      0.834\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/120     0.326G     0.8069     0.4511      1.284          8        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.5it/s 3.1s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/120     0.326G     0.7926     0.4442       1.28         16        224: 100% ━━━━━━━━━━━━ 184/184 9.2it/s 20.1s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.4it/s 3.6s0.1s\n",
      "                   all        730        736      0.997      0.992      0.995      0.825\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/120     0.326G     0.8006     0.4502      1.283         13        224: 100% ━━━━━━━━━━━━ 184/184 9.9it/s 18.5s0.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.4it/s 3.1s0.2s\n",
      "                   all        730        736      0.999      0.992      0.995      0.837\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/120     0.326G     0.7974     0.4467      1.284         16        224: 100% ━━━━━━━━━━━━ 184/184 10.0it/s 18.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.0it/s 3.9s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.831\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/120     0.326G     0.7925     0.4476      1.277         17        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.0s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.835\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/120     0.326G     0.7923     0.4395      1.273         13        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.7it/s 3.0s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.823\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/120     0.326G     0.7793     0.4336       1.26         16        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.0it/s 3.3s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.831\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/120     0.326G     0.7788     0.4287      1.265         11        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.0s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.3it/s 3.2s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.838\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/120     0.326G     0.7919     0.4433      1.276         18        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.2s\n",
      "                   all        730        736      0.999       0.99      0.995      0.829\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/120     0.326G      0.774     0.4309      1.265         16        224: 100% ━━━━━━━━━━━━ 184/184 10.6it/s 17.3s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.3it/s 3.7s0.2s\n",
      "                   all        730        736      0.999      0.992      0.995      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/120     0.326G     0.7855      0.434      1.271         18        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.9it/s 3.3s0.1s\n",
      "                   all        730        736      0.999      0.988      0.995      0.831\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/120     0.326G     0.7772     0.4273      1.266         18        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.8s.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.834\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/120     0.326G     0.7718      0.426      1.263         18        224: 100% ━━━━━━━━━━━━ 184/184 10.9it/s 16.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.2it/s 3.2s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.829\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/120     0.326G       0.77     0.4317      1.261         20        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.7it/s 3.4s0.2s\n",
      "                   all        730        736      0.999      0.992      0.995      0.828\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/120     0.326G     0.7605     0.4218      1.258         14        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.0it/s 3.3s0.1s\n",
      "                   all        730        736      0.997      0.992      0.995      0.828\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/120     0.326G     0.7669     0.4205      1.255         15        224: 100% ━━━━━━━━━━━━ 184/184 10.8it/s 17.0s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.3s0.1s\n",
      "                   all        730        736      0.998      0.992      0.995      0.827\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/120     0.326G     0.7512       0.42      1.242         16        224: 100% ━━━━━━━━━━━━ 184/184 11.0it/s 16.7s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/120     0.326G     0.7525     0.4173      1.246         16        224: 100% ━━━━━━━━━━━━ 184/184 10.7it/s 17.1s.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 7.1it/s 3.2s0.2s\n",
      "                   all        730        736      0.998      0.989      0.995      0.837\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 49, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "59 epochs completed in 0.346 hours.\n",
      "Optimizer stripped from D:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from D:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating D:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.233  Python-3.8.20 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 23/23 6.3it/s 3.7s0.1s\n",
      "                   all        730        736      0.999      0.992      0.995      0.838\n",
      "Speed: 0.1ms preprocess, 0.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\Final_Semester_Project\\AI_Attendance_System\\ai-ml-model\\new_approach\\runs\\detect\\train\u001b[0m\n",
      "YOLO training finished. Best weights in runs/detect/train/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"IPython\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data', type=str, default='D:\\\\Final_Semester_Project\\\\AI_Attendance_System\\\\AI_And_ML_Model\\\\DataSets\\\\Detection\\\\dataset.yaml')\n",
    "parser.add_argument('--epochs', type=int, default=120)\n",
    "parser.add_argument('--imgsz', type=int, default=224)\n",
    "parser.add_argument('--model', type=str, default='yolov8n.yaml')  \n",
    "parser.add_argument('--pretrained', action='store_true')\n",
    "parser.add_argument('--device', type=str, default='0')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Create data.yaml if not exists\n",
    "if not os.path.exists(args.data):\n",
    "    data_yaml = {\n",
    "        'path': os.path.abspath('datasets/detection'),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': ''  # optional\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(args.data), exist_ok=True)\n",
    "    with open(args.data, 'w') as f:\n",
    "        yaml.dump(data_yaml, f)\n",
    "    print('Created data yaml:', args.data)\n",
    "\n",
    "print('Training detection model with YOLOv8...')\n",
    "model = YOLO(args.model)\n",
    "train_kwargs = {\n",
    "    'data': args.data,\n",
    "    'epochs': args.epochs,\n",
    "    'imgsz': args.imgsz,\n",
    "    'batch': 16,\n",
    "    'device': args.device,\n",
    "    'workers': 4,\n",
    "    'verbose': True,\n",
    "    'save': True,\n",
    "    'exist_ok': True,\n",
    "    'pretrained': args.pretrained,\n",
    "    # Learning rate scheduling\n",
    "    'lr0': 0.01,  # initial learning rate\n",
    "    'lrf': 0.01,  # final learning rate factor (lr0 * lrf)\n",
    "    'patience': 10,  # epochs to wait for no improvement to reduce LR\n",
    "    # Optimizer settings\n",
    "    'optimizer': 'AdamW',\n",
    "    'weight_decay': 0.0005,\n",
    "    'warmup_epochs': 3.0,\n",
    "    'warmup_momentum': 0.8,\n",
    "    'warmup_bias_lr': 0.1,\n",
    "    # Cosine learning rate scheduler for smooth reduction\n",
    "    'cos_lr': True,\n",
    "}\n",
    "model.train(**train_kwargs)\n",
    "print('YOLO training finished. Best weights in runs/detect/train/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ebb1ac-22d7-4605-8d1f-9f76e80fed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from runs/detect/train/weights/best.pt\n",
      "Face detection started! Press 'q' to quit, 's' to save screenshot\n",
      "Face detection stopped.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def detect_faces_simple():\n",
    "    \"\"\"\n",
    "    Simple real-time face detection for Jupyter notebooks\n",
    "    \"\"\"\n",
    "    # Load your trained model - update this path to your actual model\n",
    "    model_path = \"runs/detect/train/weights/best.pt\"\n",
    "    \n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Please make sure the model path is correct.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 460)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 460)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 15)  # Limit to 15 FPS\n",
    "\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Face detection started! Press 'q' to quit, 's' to save screenshot\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image.\")\n",
    "            break\n",
    "        \n",
    "        # Perform detection\n",
    "        results = model(frame, conf=0.5, verbose=False)\n",
    "        \n",
    "        # Process detections\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                for box in boxes:\n",
    "                    # Get box coordinates and confidence\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                    conf = box.conf[0].cpu().numpy()\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Add confidence label\n",
    "                    label = f\"Face: {conf:.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1-10), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "        \n",
    "        # Add instructions to frame\n",
    "        cv2.putText(frame, \"Press 'q' to quit | 's' to save\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow('Face Detection - YOLOv8', frame)\n",
    "        \n",
    "        # Handle key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            # Save screenshot\n",
    "            cv2.imwrite('detection_screenshot.jpg', frame)\n",
    "            print(\"Screenshot saved as 'detection_screenshot.jpg'\")\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Face detection stopped.\")\n",
    "\n",
    "# Run the detection\n",
    "detect_faces_simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d3888-c6f6-486c-85ae-7104c6e9fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyVenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
