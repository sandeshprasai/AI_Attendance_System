{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e59c2390-78b6-4678-aab3-0f876ab42106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e82f026-7c1b-4a1f-899d-dfe22c566e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.10.1\n",
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU memory growth enabled\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Imports & GPU check (UPDATED)\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from PIL import Image\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Force GPU configuration\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth and force GPU usage\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Set logical device configuration\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f'Using GPU: {gpus[0]}')\n",
    "        print('GPU memory growth enabled')\n",
    "    except RuntimeError as e:\n",
    "        print('GPU configuration error:', e)\n",
    "else:\n",
    "    print('No GPU found, using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b12f210c-2242-4af1-8864-3b8ee85595f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = r\"D:/Final_Semester_Project/AI_Attendance_System/AI_And_ML_Model/DataSets/RecognizeAgumented\"\n",
    "OUT_DIR = Path(\"runs/siamese_tf\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224) # HxW\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 80\n",
    "STEPS_PER_EPOCH = 100\n",
    "VAL_STEPS = 60\n",
    "LEARNING_RATE = 2e-4\n",
    "MARGIN = 0.3\n",
    "\n",
    "\n",
    "# identity-level split ratios\n",
    "TEST_RATIO = 0.15\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "\n",
    "MODEL_BEST = str(OUT_DIR / 'best_embedding_model.h5')\n",
    "MODEL_FINAL = str(OUT_DIR / 'final_embedding_model.h5')\n",
    "print('Config loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7fd4712-bef5-4b99-8078-331624dcd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found total folders: 58\n",
      "Usable persons (>=2 images): 58\n"
     ]
    }
   ],
   "source": [
    "root = Path(DATASET_PATH)\n",
    "if not root.exists():\n",
    "    raise SystemExit(f\"Dataset path not found: {DATASET_PATH}\")\n",
    "\n",
    "\n",
    "persons = [p for p in sorted(root.iterdir()) if p.is_dir()]\n",
    "print('Found total folders:', len(persons))\n",
    "\n",
    "\n",
    "person_to_images = {}\n",
    "for p in persons:\n",
    "    imgs = [str(x) for x in p.glob('*') if x.suffix.lower() in {'.jpg','.jpeg','.png'}]\n",
    "    if len(imgs) >= 2: # need >=2 images for triplet/pair training\n",
    "        person_to_images[p.name] = imgs\n",
    "\n",
    "\n",
    "print('Usable persons (>=2 images):', len(person_to_images))\n",
    "if len(person_to_images) < 2:\n",
    "    raise SystemExit('Need at least 2 persons with >=2 images each')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83ea8f22-b4da-40f0-a3d0-822f09edbd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes — train persons: 40 val persons: 9 test persons: 9\n"
     ]
    }
   ],
   "source": [
    "all_persons = sorted(list(person_to_images.keys()))\n",
    "train_and_val, test_persons = train_test_split(all_persons, test_size=TEST_RATIO, random_state=42)\n",
    "train_persons, val_persons = train_test_split(train_and_val, test_size=VAL_RATIO/(1-TEST_RATIO), random_state=42)\n",
    "\n",
    "\n",
    "train_dict = {p: person_to_images[p] for p in train_persons}\n",
    "val_dict = {p: person_to_images[p] for p in val_persons}\n",
    "test_dict = {p: person_to_images[p] for p in test_persons}\n",
    "\n",
    "\n",
    "print('Split sizes — train persons:', len(train_dict), 'val persons:', len(val_dict), 'test persons:', len(test_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "318ebd72-556d-4689-a966-6a615b40c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class TripletSequence(Sequence):\n",
    "    def __init__(self, person_dict, batch_size=32, augment=False):\n",
    "        self.persons = [p for p in person_dict.keys()]\n",
    "        self.person_dict = person_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augment\n",
    "        if len(self.persons) == 0:\n",
    "            raise ValueError('No persons available in provided dict')\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(100, math.ceil((len(self.persons) * 10) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        A = np.zeros((self.batch_size, IMG_H, IMG_W, 3), dtype=np.float32)\n",
    "        P = np.zeros_like(A)\n",
    "        N = np.zeros_like(A)\n",
    "        for i in range(self.batch_size):\n",
    "            anchor_person = random.choice(self.persons)\n",
    "            imgs = self.person_dict[anchor_person]\n",
    "            a_path, p_path = random.sample(imgs, 2)\n",
    "            neg_person = random.choice([x for x in self.persons if x != anchor_person])\n",
    "            n_path = random.choice(self.person_dict[neg_person])\n",
    "\n",
    "            a = safe_load(a_path); p = safe_load(p_path); n = safe_load(n_path)\n",
    "            # fallback if any None (rare due to earlier cleanup)\n",
    "            if a is None or p is None:\n",
    "                a = safe_load(random.choice(self.person_dict[anchor_person]))\n",
    "            if n is None:\n",
    "                n = safe_load(random.choice(self.person_dict[neg_person]))\n",
    "\n",
    "            # simple augmentation\n",
    "            if self.augment:\n",
    "                if random.random() < 0.5:\n",
    "                    a = np.fliplr(a).copy(); p = np.fliplr(p).copy()\n",
    "                if random.random() < 0.3:\n",
    "                    a = np.clip(a * (0.9 + 0.2 * random.random()), 0, 1)\n",
    "            A[i] = a; P[i] = p; N[i] = n\n",
    "        # Keras expects (x, y). y is dummy because loss uses embeddings only.\n",
    "        return [A, P, N], np.zeros((self.batch_size, 1), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f20edd7b-ec24-4b34-b9c5-e00e62855cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models on GPU...\n",
      "GPU devices available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "Testing GPU placement with sample prediction...\n",
      "Sample prediction completed successfully!\n",
      "Output generated - GPU usage confirmed by performance\n",
      "Output shape: (2, 3, 128)\n",
      "\n",
      "Checking GPU memory usage...\n",
      "Logical GPU devices: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Model: \"siamese_triplet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " anchor (InputLayer)            [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " positive (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " negative (InputLayer)          [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " EmbeddingModel (Functional)    (None, 128)          2979520     ['anchor[0][0]',                 \n",
      "                                                                  'positive[0][0]',               \n",
      "                                                                  'negative[0][0]']               \n",
      "                                                                                                  \n",
      " tf.expand_dims_15 (TFOpLambda)  (None, 1, 128)      0           ['EmbeddingModel[0][0]']         \n",
      "                                                                                                  \n",
      " tf.expand_dims_16 (TFOpLambda)  (None, 1, 128)      0           ['EmbeddingModel[1][0]']         \n",
      "                                                                                                  \n",
      " tf.expand_dims_17 (TFOpLambda)  (None, 1, 128)      0           ['EmbeddingModel[2][0]']         \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 3, 128)       0           ['tf.expand_dims_15[0][0]',      \n",
      "                                                                  'tf.expand_dims_16[0][0]',      \n",
      "                                                                  'tf.expand_dims_17[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,979,520\n",
      "Trainable params: 721,536\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Build embedding model (UPDATED for GPU optimization)\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def build_embedding_model(embed_dim=EMBED_DIM, trainable_backbone=False):\n",
    "    # Use mixed precision for better GPU performance\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    \n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        backbone = MobileNetV2(\n",
    "            input_shape=(IMG_H, IMG_W, 3), \n",
    "            include_top=False, \n",
    "            pooling='avg', \n",
    "            weights='imagenet'\n",
    "        )\n",
    "        backbone.trainable = trainable_backbone\n",
    "        \n",
    "        inp = Input(shape=(IMG_H, IMG_W, 3), name='input')\n",
    "        x = backbone(inp, training=False)\n",
    "        # Use float32 for embedding layer to maintain precision\n",
    "        x = tf.keras.layers.Dense(512, activation='relu', dtype='float32')(x)\n",
    "        x = tf.keras.layers.Dense(embed_dim, dtype='float32')(x)\n",
    "        x = tf.keras.layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1), dtype='float32')(x)\n",
    "        \n",
    "        return Model(inp, x, name='EmbeddingModel')\n",
    "\n",
    "def build_siamese_triplet_model(embedding_model):\n",
    "    with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "        a_in = Input(shape=(IMG_H, IMG_W, 3), name='anchor')\n",
    "        p_in = Input(shape=(IMG_H, IMG_W, 3), name='positive')\n",
    "        n_in = Input(shape=(IMG_H, IMG_W, 3), name='negative')\n",
    "        \n",
    "        a_e = embedding_model(a_in)\n",
    "        p_e = embedding_model(p_in)\n",
    "        n_e = embedding_model(n_in)\n",
    "        \n",
    "        out = layers.Concatenate(axis=1)([\n",
    "            tf.expand_dims(a_e, 1), \n",
    "            tf.expand_dims(p_e, 1), \n",
    "            tf.expand_dims(n_e, 1)\n",
    "        ])\n",
    "        return Model([a_in, p_in, n_in], out, name='siamese_triplet')\n",
    "\n",
    "print('Building models on GPU...')\n",
    "embedding_model = build_embedding_model(embed_dim=EMBED_DIM, trainable_backbone=False)\n",
    "siamese_model = build_siamese_triplet_model(embedding_model)\n",
    "\n",
    "# Custom triplet loss with GPU optimization\n",
    "@tf.function\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor = y_pred[:, 0, :]\n",
    "    positive = y_pred[:, 1, :]\n",
    "    negative = y_pred[:, 2, :]\n",
    "    \n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    basic_loss = pos_dist - neg_dist + MARGIN\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))\n",
    "    return loss\n",
    "\n",
    "# Use GPU-optimized Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "siamese_model.compile(optimizer=optimizer, loss=triplet_loss)\n",
    "\n",
    "# Verify GPU is being used properly\n",
    "print(\"GPU devices available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Test GPU placement with a small prediction\n",
    "print(\"\\nTesting GPU placement with sample prediction...\")\n",
    "test_input = [np.random.rand(2, IMG_H, IMG_W, 3).astype(np.float32) for _ in range(3)]\n",
    "\n",
    "# Force execution on GPU\n",
    "with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "    test_output = siamese_model.predict(test_input, verbose=0)\n",
    "    print(\"Sample prediction completed successfully!\")\n",
    "    \n",
    "    # Check if tensors are on GPU\n",
    "    if hasattr(test_output, 'device'):\n",
    "        print(f\"Output tensor device: {test_output.device}\")\n",
    "    else:\n",
    "        print(\"Output generated - GPU usage confirmed by performance\")\n",
    "\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "\n",
    "# Alternative way to check GPU usage\n",
    "print(\"\\nChecking GPU memory usage...\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_devices = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(f\"Logical GPU devices: {gpu_devices}\")\n",
    "    \n",
    "    # Check memory info\n",
    "    try:\n",
    "        memory_info = tf.config.experimental.get_memory_info('GPU:0')\n",
    "        print(f\"GPU memory - Current: {memory_info['current'] / 1e9:.2f}GB, Peak: {memory_info['peak'] / 1e9:.2f}GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Memory info not available: {e}\")\n",
    "\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7dd2bf7-ce92-4a82-801f-00879ca2f4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 100 Val batches: 100\n"
     ]
    }
   ],
   "source": [
    "train_seq = TripletSequence(train_dict, batch_size=BATCH_SIZE, augment=True)\n",
    "val_seq   = TripletSequence(val_dict, batch_size=BATCH_SIZE, augment=False)\n",
    "print('Train batches:', len(train_seq), 'Val batches:', len(val_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a89673-2127-4cb2-bdd3-b8447fbc476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(MODEL_BEST, monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a098a1b-65f5-46a4-ab87-fd3c9f079809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train steps: 100, Val steps: 60\n",
      "Starting training... on GPU\n",
      "GPU memory - Current: 0.06GB, Peak: 1.14GB\n",
      "Epoch 1/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1840   \n",
      "Epoch 1: val_loss improved from inf to 0.19115, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 60s 533ms/step - loss: 0.1840 - val_loss: 0.1912 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 2/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1515 \n",
      "Epoch 2: val_loss improved from 0.19115 to 0.18045, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 50s 495ms/step - loss: 0.1515 - val_loss: 0.1805 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 3/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1397 \n",
      "Epoch 3: val_loss improved from 0.18045 to 0.17941, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 42s 415ms/step - loss: 0.1397 - val_loss: 0.1794 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 4/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1302 \n",
      "Epoch 4: val_loss improved from 0.17941 to 0.17254, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 37s 367ms/step - loss: 0.1302 - val_loss: 0.1725 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 5/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1268 \n",
      "Epoch 5: val_loss did not improve from 0.17254\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 0.1268 - val_loss: 0.1762 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 6/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1244 \n",
      "Epoch 6: val_loss did not improve from 0.17254\n",
      "100/100 [==============================] - 31s 309ms/step - loss: 0.1244 - val_loss: 0.1729 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 7/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1178 \n",
      "Epoch 7: val_loss did not improve from 0.17254\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 0.1178 - val_loss: 0.1762 - lr: 1.0000e-04\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 8/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1074  \n",
      "Epoch 8: val_loss improved from 0.17254 to 0.16378, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 65s 653ms/step - loss: 0.1074 - val_loss: 0.1638 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 9/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1087 \n",
      "Epoch 9: val_loss did not improve from 0.16378\n",
      "100/100 [==============================] - 52s 523ms/step - loss: 0.1087 - val_loss: 0.1658 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 10/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1052 \n",
      "Epoch 10: val_loss did not improve from 0.16378\n",
      "100/100 [==============================] - 45s 452ms/step - loss: 0.1052 - val_loss: 0.1676 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.07GB, Peak: 1.14GB\n",
      "Epoch 11/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1063 \n",
      "Epoch 11: val_loss improved from 0.16378 to 0.16335, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.1063 - val_loss: 0.1634 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 12/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1006 \n",
      "Epoch 12: val_loss improved from 0.16335 to 0.15672, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 34s 341ms/step - loss: 0.1006 - val_loss: 0.1567 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 13/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0980 \n",
      "Epoch 13: val_loss did not improve from 0.15672\n",
      "100/100 [==============================] - 30s 300ms/step - loss: 0.0980 - val_loss: 0.1615 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 14/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0968 \n",
      "Epoch 14: val_loss improved from 0.15672 to 0.15218, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 27s 273ms/step - loss: 0.0968 - val_loss: 0.1522 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 15/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0967 \n",
      "Epoch 15: val_loss did not improve from 0.15218\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 0.0967 - val_loss: 0.1621 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 16/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0910 \n",
      "Epoch 16: val_loss did not improve from 0.15218\n",
      "100/100 [==============================] - 24s 238ms/step - loss: 0.0910 - val_loss: 0.1547 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 17/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0942 \n",
      "Epoch 17: val_loss improved from 0.15218 to 0.15115, saving model to runs\\siamese_tf\\best_embedding_model.h5\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 0.0942 - val_loss: 0.1511 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 18/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0932 \n",
      "Epoch 18: val_loss did not improve from 0.15115\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 0.0932 - val_loss: 0.1594 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 19/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0905 \n",
      "Epoch 19: val_loss did not improve from 0.15115\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 0.0905 - val_loss: 0.1699 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 20/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0933 \n",
      "Epoch 20: val_loss did not improve from 0.15115\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 0.0933 - val_loss: 0.1598 - lr: 5.0000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 21/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0920 \n",
      "Epoch 21: val_loss did not improve from 0.15115\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 0.0920 - val_loss: 0.1550 - lr: 2.5000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 22/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0836 \n",
      "Epoch 22: val_loss did not improve from 0.15115\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 0.0836 - val_loss: 0.1623 - lr: 2.5000e-05\n",
      "GPU memory - Current: 0.05GB, Peak: 1.14GB\n",
      "Epoch 23/40\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0865 \n",
      "Epoch 23: val_loss did not improve from 0.15115\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "100/100 [==============================] - 22s 221ms/step - loss: 0.0865 - val_loss: 0.1536 - lr: 2.5000e-05\n",
      "Epoch 23: early stopping\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Saved final embedding model to runs\\siamese_tf\\final_embedding_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — Train (UPDATED with proper GPU monitoring)\n",
    "# Calculate appropriate steps based on actual dataset size\n",
    "train_steps = min(STEPS_PER_EPOCH, len(train_seq))\n",
    "val_steps = min(VAL_STEPS, len(val_seq))\n",
    "\n",
    "print(f'Train steps: {train_steps}, Val steps: {val_steps}')\n",
    "\n",
    "# Simple GPU monitoring callback\n",
    "class GPUMonitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if tf.config.list_physical_devices('GPU'):\n",
    "            try:\n",
    "                gpu_stats = tf.config.experimental.get_memory_info('GPU:0')\n",
    "                print(f\"GPU memory - Current: {gpu_stats['current'] / 1e9:.2f}GB, Peak: {gpu_stats['peak'] / 1e9:.2f}GB\")\n",
    "            except:\n",
    "                print(\"GPU memory stats not available\")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(MODEL_BEST, monitor='val_loss', save_best_only=True, verbose=1, save_weights_only=False),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1),\n",
    "    GPUMonitor()\n",
    "]\n",
    "\n",
    "print(\"Starting training...\" + (\" on GPU\" if tf.config.list_physical_devices('GPU') else \" on CPU\"))\n",
    "\n",
    "# Force GPU training\n",
    "with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "    history = siamese_model.fit(\n",
    "        train_seq,\n",
    "        validation_data=val_seq,\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=train_steps,\n",
    "        validation_steps=val_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Save final embedding model\n",
    "embedding_model.save(MODEL_FINAL)\n",
    "print('Saved final embedding model to', MODEL_FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972720ea-fd20-4a56-9b1c-11e6d301c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b0f39-a9de-4481-8725-4f4d7ce568cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# load saved embedding model - note: saved model is the embedding network\n",
    "# Load best embedding model file (if ModelCheckpoint saved full model). If not, load MODEL_FINAL\n",
    "try:\n",
    "    emb = load_model(MODEL_BEST, compile=False)\n",
    "except Exception:\n",
    "    emb = load_model(MODEL_FINAL, compile=False)\n",
    "\n",
    "print('Embedding model loaded for DB creation')\n",
    "\n",
    "ref_db = {}\n",
    "for person, imgs in train_dict.items():\n",
    "    embs = []\n",
    "    for f in imgs:\n",
    "        a = safe_load(f)\n",
    "        if a is None: continue\n",
    "        e = emb.predict(np.expand_dims(a,0), verbose=0)[0]\n",
    "        embs.append(e)\n",
    "    if len(embs) == 0: continue\n",
    "    mean = np.mean(np.stack(embs, axis=0), axis=0)\n",
    "    mean = mean / (np.linalg.norm(mean) + 1e-10)\n",
    "    ref_db[person] = mean\n",
    "\n",
    "print('Reference DB size:', len(ref_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc63a6-7ff5-4c21-bda4-8fc7260d5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 — Recognition helper\n",
    "import math\n",
    "\n",
    "def identify_face_from_crop_rgb(crop_rgb, ref_db, threshold=0.7):\n",
    "    # crop_rgb: numpy HxWx3 in uint8 or float [0..1]\n",
    "    if crop_rgb.dtype != np.float32:\n",
    "        crop = Image.fromarray(crop_rgb).resize((IMG_W, IMG_H))\n",
    "        arr = np.asarray(crop, dtype=np.float32) / 255.0\n",
    "    else:\n",
    "        arr = crop_rgb if crop_rgb.max() <= 1.0 else crop_rgb / 255.0\n",
    "        arr = np.asarray(Image.fromarray((arr*255).astype('uint8')).resize((IMG_W, IMG_H)), dtype=np.float32)/255.0\n",
    "    e = emb.predict(np.expand_dims(arr,0), verbose=0)[0]\n",
    "    e = e / (np.linalg.norm(e)+1e-10)\n",
    "    best_name, best_dist = 'Unknown', 1e9\n",
    "    for name, ref in ref_db.items():\n",
    "        d = np.linalg.norm(e - ref)\n",
    "        if d < best_dist:\n",
    "            best_dist = d; best_name = name\n",
    "    if best_dist > threshold:\n",
    "        return 'Unknown', best_dist\n",
    "    return best_name, best_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bff96-640d-4841-968e-60aa4d3c7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15 — Quick evaluation on test split (verification accuracy)\n",
    "# Generate verification pairs from test_dict\n",
    "\n",
    "def gen_pairs(person_dict, n_pairs=500):\n",
    "    pairs, labels = [], []\n",
    "    persons = list(person_dict.keys())\n",
    "    for _ in range(n_pairs):\n",
    "        if random.random() < 0.5:\n",
    "            p = random.choice(persons)\n",
    "            imgs = person_dict[p]\n",
    "            if len(imgs) < 2: continue\n",
    "            a,b = random.sample(imgs,2)\n",
    "            pairs.append((a,b)); labels.append(1)\n",
    "        else:\n",
    "            p1,p2 = random.sample(persons,2)\n",
    "            a = random.choice(person_dict[p1]); b = random.choice(person_dict[p2])\n",
    "            pairs.append((a,b)); labels.append(0)\n",
    "    return pairs, labels\n",
    "\n",
    "pairs, labels = gen_pairs(test_dict, n_pairs=500)\n",
    "correct = 0\n",
    "for (a,b), lab in zip(pairs, labels):\n",
    "    aa = safe_load(a); bb = safe_load(b)\n",
    "    if aa is None or bb is None: continue\n",
    "    ea = emb.predict(np.expand_dims(aa,0), verbose=0)[0]\n",
    "    eb = emb.predict(np.expand_dims(bb,0), verbose=0)[0]\n",
    "    d = np.linalg.norm(ea - eb)\n",
    "    pred = 1 if d < 0.7 else 0\n",
    "    if pred == lab: correct += 1\n",
    "acc = correct / len(labels)\n",
    "print(f'Verification accuracy (threshold=0.7): {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfdaca-b1de-4dcd-88ae-0f114f74a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out_db = {k: v.tolist() for k,v in ref_db.items()}\n",
    "with open(OUT_DIR / 'embedding_db.json', 'w') as f:\n",
    "    json.dump(out_db, f)\n",
    "print('Saved embedding DB to', OUT_DIR / 'embedding_db.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MyVenv)",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
