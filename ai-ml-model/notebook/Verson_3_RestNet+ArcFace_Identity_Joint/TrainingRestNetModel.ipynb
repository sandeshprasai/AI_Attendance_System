{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14459907,"sourceType":"datasetVersion","datasetId":9235761}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nimport logging\n\n# ============================================================================\n# SUPPRESS ALL TENSORFLOW WARNINGS AND CUDA MESSAGES\n# ============================================================================\n\n# 1. Suppress TensorFlow logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0=ALL, 1=INFO, 2=WARNING, 3=ERROR\n\n# 2. Suppress CUDA/cuDNN warnings\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\n# 3. Suppress XLA compilation messages\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices=false'\n\n# 4. Suppress Python warnings\nwarnings.filterwarnings('ignore')\n\n# 5. Set logging level for various libraries\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\nlogging.getLogger('absl').setLevel(logging.ERROR)\n\n# 6. Disable deprecation warnings\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\nprint(\"✅ All warnings suppressed - training output will be clean!\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:23.996911Z","iopub.execute_input":"2026-02-01T08:28:23.997112Z","iopub.status.idle":"2026-02-01T08:28:38.730533Z","shell.execute_reply.started":"2026-02-01T08:28:23.997092Z","shell.execute_reply":"2026-02-01T08:28:38.729753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Import all required libraries\nimport os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import (\n    roc_curve, auc, precision_recall_curve, average_precision_score,\n    confusion_matrix, classification_report, f1_score,\n    accuracy_score, precision_score, recall_score,\n    roc_auc_score\n)\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import stats\nimport itertools\nfrom sklearn.calibration import calibration_curve\nfrom tqdm import tqdm\nimport warnings\nimport math\nfrom scipy.spatial.distance import pdist, squareform\nfrom itertools import combinations\nfrom sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\nimport pickle\nimport json\n\n# TensorFlow imports for model\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"✅ All libraries imported successfully\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:38.732104Z","iopub.execute_input":"2026-02-01T08:28:38.732608Z","iopub.status.idle":"2026-02-01T08:28:39.533521Z","shell.execute_reply.started":"2026-02-01T08:28:38.732581Z","shell.execute_reply":"2026-02-01T08:28:39.532804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Global configuration\n# Global config\nIMG_SIZE = 112\nBATCH_SIZE = 64  # Increase for better gradient estimates\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 42\n\n# Add augmentation flag\nUSE_AUGMENTATION = True  # Add data augmentation to prevent overfitting\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\nDATASET_DIR = \"/kaggle/input/datasetsforrestnet/ThirdLap\"\n\nprint(\"✅ Configuration loaded\")\nprint(f\"Image Size: {IMG_SIZE}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Dataset Directory: {DATASET_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:39.534361Z","iopub.execute_input":"2026-02-01T08:28:39.534868Z","iopub.status.idle":"2026-02-01T08:28:39.540878Z","shell.execute_reply.started":"2026-02-01T08:28:39.534844Z","shell.execute_reply":"2026-02-01T08:28:39.539994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def parse_image_with_augmentation(file_path, label, training=True):\n    \"\"\"Parse and preprocess image with optional augmentation\"\"\"\n    image = tf.io.read_file(file_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    \n    if training and USE_AUGMENTATION:\n        # Data augmentation using only TensorFlow built-ins\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_brightness(image, 0.2)\n        image = tf.image.random_contrast(image, 0.8, 1.2)\n        image = tf.image.random_saturation(image, 0.8, 1.2)\n        image = tf.image.random_hue(image, 0.1)\n        \n        # Random crop and resize (simulates slight rotation/zoom)\n        crop_size = tf.random.uniform([], 0.9, 1.0)\n        crop_h = tf.cast(IMG_SIZE * crop_size, tf.int32)\n        crop_w = tf.cast(IMG_SIZE * crop_size, tf.int32)\n        image = tf.image.random_crop(image, [crop_h, crop_w, 3])\n        image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    \n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.clip_by_value(image, 0.0, 1.0)\n    \n    return image, label\n\n\ndef build_train_val_datasets_with_augmentation(root_dir, val_split=0.2):\n    \"\"\"\n    Build training and validation datasets with proper split.\n    Each identity appears in BOTH train and val sets - THIS IS THE KEY FIX!\n    \"\"\"\n    class_names = sorted([d for d in os.listdir(root_dir)\n                         if os.path.isdir(os.path.join(root_dir, d))])\n    \n    num_classes = len(class_names)\n    print(f\"Total identities: {num_classes}\")\n    \n    train_paths, train_labels = [], []\n    val_paths, val_labels = [], []\n    \n    # For each identity, split its images into train/val\n    for idx, cls in enumerate(class_names):\n        cls_dir = os.path.join(root_dir, cls)\n        images = [os.path.join(cls_dir, img) \n                 for img in os.listdir(cls_dir)\n                 if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n        \n        # Shuffle images for this class\n        random.shuffle(images)\n        \n        # Split: 80% train, 20% val (at least 1 image for val)\n        n_val = max(1, int(len(images) * val_split))\n        val_images = images[:n_val]\n        train_images = images[n_val:]\n        \n        # Same label (idx) for both train and val!\n        val_paths.extend(val_images)\n        val_labels.extend([idx] * len(val_images))\n        \n        train_paths.extend(train_images)\n        train_labels.extend([idx] * len(train_images))\n    \n    print(f\"\\nDataset Statistics:\")\n    print(f\"  Total classes: {num_classes}\")\n    print(f\"  Training images: {len(train_paths)}\")\n    print(f\"  Validation images: {len(val_paths)}\")\n    print(f\"  Train/Val split: {100*(1-val_split):.0f}% / {100*val_split:.0f}%\\n\")\n    \n    # Create datasets\n    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n    train_ds = train_ds.shuffle(10000, seed=SEED)\n    train_ds = train_ds.map(\n        lambda x, y: parse_image_with_augmentation(x, y, training=True), \n        num_parallel_calls=AUTOTUNE\n    )\n    train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n    val_ds = val_ds.map(\n        lambda x, y: parse_image_with_augmentation(x, y, training=False),\n        num_parallel_calls=AUTOTUNE\n    )\n    val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n    \n    print(\"✅ Datasets created successfully\\n\")\n    return train_ds, val_ds, num_classes, class_names\n\nprint(\"✅ Data loading functions defined\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:39.541861Z","iopub.execute_input":"2026-02-01T08:28:39.542141Z","iopub.status.idle":"2026-02-01T08:28:39.560603Z","shell.execute_reply.started":"2026-02-01T08:28:39.542120Z","shell.execute_reply":"2026-02-01T08:28:39.559756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading datasets...\")\ntrain_ds, val_ds, num_classes, all_class_names= build_train_val_datasets_with_augmentation(\n    root_dir=DATASET_DIR,\n    val_split=0.2  # 20% of images from each identity for validation\n)\n\nprint(f\"\\nFinal numbers for model:\")\nprint(f\"Number of classes: {num_classes}\")\n\n# Verify dataset\nsample_batch = next(iter(train_ds))\nprint(f\"Training batch shape: {sample_batch[0].shape}\")\nprint(f\"Labels shape: {sample_batch[1].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:39.561714Z","iopub.execute_input":"2026-02-01T08:28:39.561994Z","iopub.status.idle":"2026-02-01T08:28:48.631114Z","shell.execute_reply.started":"2026-02-01T08:28:39.561972Z","shell.execute_reply":"2026-02-01T08:28:48.630174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Define model architecture for face verification\ndef l2_norm(x):\n    return tf.nn.l2_normalize(x, axis=1)\n\ndef build_resnet_embedding():\n    \"\"\"Build ResNet50 backbone for embeddings\"\"\"\n    inputs = Input(shape=(112, 112, 3), name=\"input_image\")\n    \n    base = ResNet50(\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=inputs,\n        pooling=\"avg\"\n    )\n    \n    x = BatchNormalization()(base.output)\n    x = Dense(512, use_bias=False)(x)\n    x = BatchNormalization()(x)\n    embeddings = Lambda(l2_norm, name=\"embeddings\")(x)\n    \n    return Model(inputs, embeddings, name=\"ResNet50_Embedding\")\n\nclass ArcFace(tf.keras.layers.Layer):\n    \"\"\"ArcFace layer implementation - only for training\"\"\"\n    def __init__(self, num_classes, margin=0.5, scale=64, **kwargs):\n        super(ArcFace, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.margin = margin\n        self.scale = scale\n        self.cos_m = tf.math.cos(margin)\n        self.sin_m = tf.math.sin(margin)\n        self.threshold = tf.math.cos(math.pi - margin)\n        self.cos_mt = tf.math.cos(math.pi - margin)\n        \n    def build(self, input_shape):\n        self.W = self.add_weight(\n            name=\"W\",\n            shape=(input_shape[-1], self.num_classes),\n            initializer=\"glorot_uniform\",\n            trainable=True\n        )\n        \n    def call(self, inputs, labels=None):\n        # Normalize weights and inputs\n        W_norm = tf.nn.l2_normalize(self.W, axis=0)\n        x_norm = tf.nn.l2_normalize(inputs, axis=1)\n        \n        # Compute cosine similarity\n        cosine = tf.matmul(x_norm, W_norm)\n        \n        if labels is not None:\n            # Create one-hot encoded labels\n            one_hot_labels = tf.one_hot(labels, depth=self.num_classes)\n            \n            # Get cosine of correct classes\n            cos_y = tf.reduce_sum(one_hot_labels * cosine, axis=1, keepdims=True)\n            \n            # Compute sin\n            sin_y = tf.math.sqrt(1.0 - tf.math.square(cos_y))\n            \n            # Compute cos(theta + margin)\n            cos_theta_m = cos_y * self.cos_m - sin_y * self.sin_m\n            \n            # Apply margin\n            cos_theta_m = tf.where(cos_y > self.threshold, cos_theta_m, cos_y - self.cos_mt)\n            \n            # Scale and apply\n            output = cosine + one_hot_labels * (cos_theta_m - cos_y)\n            output = output * self.scale\n            return output\n        else:\n            return cosine * self.scale\n\nprint(\"✅ Model architecture defined (face verification version)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:48.632317Z","iopub.execute_input":"2026-02-01T08:28:48.632666Z","iopub.status.idle":"2026-02-01T08:28:48.643539Z","shell.execute_reply.started":"2026-02-01T08:28:48.632635Z","shell.execute_reply":"2026-02-01T08:28:48.642868Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Build and compile model for face verification (FIXED)\n\nprint(\"Building face verification model...\")\n\n# Build backbone\nbackbone = build_resnet_embedding()\nprint(\"Backbone summary:\")\nbackbone.summary()\n\n# Build ArcFace layer for training\narcface = ArcFace(num_classes, margin=0.5, scale=64)\n\n# Create training model\ninputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\nlabels = Input(shape=(), name=\"label\", dtype=tf.int32)\n\nembeddings = backbone(inputs)\nlogits = arcface(embeddings, labels, training=True)\n\n# Training model\ntrain_model = Model([inputs, labels], logits, name=\"ArcFace_Trainer\")\n\n# ============================================================================\n# FIX: Use EITHER LearningRateSchedule OR ReduceLROnPlateau, NOT BOTH\n# ============================================================================\n\n# OPTION 1: Simple fixed learning rate + ReduceLROnPlateau (RECOMMENDED)\n# This is simpler and works well with the ReduceLROnPlateau callback\n\ntrain_model.compile(\n    optimizer=Adam(learning_rate=1e-4),  # Fixed learning rate\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nprint(\"✅ Training model compiled with fixed LR (will be adjusted by ReduceLROnPlateau)\")\n\n\n# OPTION 2: Use LearningRateSchedule WITHOUT ReduceLROnPlateau\n# Uncomment this and comment out Option 1 if you prefer a schedule\n# Also need to remove ReduceLROnPlateau from callbacks in Cell 8\n\n\"\"\"\nlr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-4,\n    decay_steps=EPOCHS * (len(train_paths) // BATCH_SIZE),\n    alpha=1e-6\n)\n\ntrain_model.compile(\n    optimizer=Adam(learning_rate=lr_schedule),\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\nprint(\"✅ Training model compiled with CosineDecay schedule\")\nprint(\"NOTE: Remove ReduceLROnPlateau from callbacks!\")\n\"\"\"\n\n\n# Build model with input shape\ntrain_model.build([(None, IMG_SIZE, IMG_SIZE, 3), (None,)])\n\nprint(f\"Total parameters: {train_model.count_params():,}\")\n\n# Create inference model (for extracting embeddings)\ninference_model = Model(inputs=backbone.input, outputs=backbone.output, \n                       name=\"Face_Embedding_Model\")\nprint(\"\\n✅ Inference model for embedding extraction created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:48.645977Z","iopub.execute_input":"2026-02-01T08:28:48.646302Z","iopub.status.idle":"2026-02-01T08:28:51.411333Z","shell.execute_reply.started":"2026-02-01T08:28:48.646282Z","shell.execute_reply":"2026-02-01T08:28:51.410692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Prepare datasets for model training\nprint(\"Preparing datasets for training...\")\n\n# The training model expects [images, labels] as input\ndef prepare_train_dataset(dataset):\n    \"\"\"Convert dataset to format expected by training model\"\"\"\n    def map_fn(images, labels):\n        return (images, labels), labels\n    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE)\n\ndef prepare_val_dataset(dataset):\n    \"\"\"Convert validation dataset for training model\"\"\"\n    def map_fn(images, labels):\n        return (images, labels), labels\n    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE)\n\n# Prepare datasets\ntrain_ds_for_model = prepare_train_dataset(train_ds)\nval_ds_for_model = prepare_val_dataset(val_ds)\n\n# Use cardinality instead of iterating through the entire dataset\ntrain_card = tf.data.experimental.cardinality(train_ds_for_model).numpy()\nval_card   = tf.data.experimental.cardinality(val_ds_for_model).numpy()\nprint(f\"Training batches  : {train_card}\")\nprint(f\"Validation batches: {val_card}\")\nprint(\"✅ Datasets prepared for ArcFace training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:51.412135Z","iopub.execute_input":"2026-02-01T08:28:51.412391Z","iopub.status.idle":"2026-02-01T08:28:51.465335Z","shell.execute_reply.started":"2026-02-01T08:28:51.412371Z","shell.execute_reply":"2026-02-01T08:28:51.464690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Define training callbacks with embedding saver\nprint(\"Setting up callbacks...\")\n\ndef compute_similarity_scores_for_auc(embeddings, labels):\n    \"\"\"Compute similarity scores for ROC-AUC calculation (memory-safe)\"\"\"\n    n = len(embeddings)\n    if n > 1000:\n        indices = np.random.choice(n, 1000, replace=False)\n        embeddings = embeddings[indices]\n        labels = labels[indices]\n        n = 1000\n\n    similarity_matrix = sklearn_cosine_similarity(embeddings)\n\n    max_pairs = min(10000, n * (n - 1) // 2)\n    pair_labels = []\n    scores = []\n    pairs_sampled = 0\n\n    while pairs_sampled < max_pairs:\n        i, j = np.random.randint(0, n, 2)\n        if i == j:\n            continue\n        scores.append(similarity_matrix[i, j])\n        pair_labels.append(1 if labels[i] == labels[j] else 0)\n        pairs_sampled += 1\n\n    del similarity_matrix  # free immediately\n\n    return np.array(scores), np.array(pair_labels)\n\n\nclass EmbeddingSaverCallback(tf.keras.callbacks.Callback):\n    \"\"\"Saves only the ROC-AUC per epoch — does NOT accumulate embeddings in RAM.\"\"\"\n    def __init__(self, backbone, val_ds, save_dir='embeddings_by_epoch'):\n        super().__init__()\n        self.backbone = backbone\n        self.val_ds = val_ds\n        self.save_dir = save_dir\n        os.makedirs(save_dir, exist_ok=True)\n        self.epoch_roc_aucs = []  # only a short list of floats\n\n        # Extract labels once (cheap — just ints)\n        print(\"Extracting validation labels...\")\n        all_labels = []\n        for images, labels in self.val_ds:\n            all_labels.append(labels.numpy())\n        self.val_labels = np.concatenate(all_labels)\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"\\nEpoch {epoch+1}: Extracting embeddings for ROC-AUC...\")\n\n        # Stream embeddings batch-by-batch\n        embeddings_list = []\n        for images, _ in self.val_ds:\n            emb = self.backbone(images, training=False)\n            emb = tf.math.l2_normalize(emb, axis=1)\n            embeddings_list.append(emb.numpy())\n\n        embeddings = np.vstack(embeddings_list)\n        del embeddings_list  # release list of arrays\n\n        # Compute ROC-AUC (internally subsamples to 1000 samples)\n        scores, pair_labels = compute_similarity_scores_for_auc(embeddings, self.val_labels)\n        del embeddings  # free the big array NOW\n\n        if len(np.unique(pair_labels)) > 1:\n            roc_auc = roc_auc_score(pair_labels, scores)\n            self.epoch_roc_aucs.append(roc_auc)\n            if logs is not None:\n                logs['val_roc_auc'] = roc_auc\n            print(f\"Epoch {epoch+1}: ROC-AUC = {roc_auc:.4f}\")\n        else:\n            self.epoch_roc_aucs.append(0.5)\n\n        del scores, pair_labels  # free\n\n        # Persist only the lightweight history list\n        np.save(os.path.join(self.save_dir, 'roc_auc_history.npy'),\n                np.array(self.epoch_roc_aucs))\n\n        # Persist a small JSON summary (no large arrays)\n        summary = {\n            'epoch': epoch + 1,\n            'val_loss': float(logs.get('val_loss', 0)),\n            'val_accuracy': float(logs.get('val_accuracy', 0)),\n            'val_roc_auc': float(logs.get('val_roc_auc', 0.5)),\n            'loss': float(logs.get('loss', 0)),\n            'accuracy': float(logs.get('accuracy', 0))\n        }\n        with open(os.path.join(self.save_dir, f'epoch_{epoch+1:03d}_summary.json'), 'w') as f:\n            json.dump(summary, f, indent=4)\n\n\n# Create the embedding saver callback\nembedding_saver = EmbeddingSaverCallback(backbone, val_ds, save_dir='embeddings_by_epoch')\n\n# Define all callbacks\ncallbacks = [\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    EarlyStopping(\n        monitor='val_loss',\n        patience=5,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        'best_arcface_model.h5',\n        monitor='val_loss',\n        save_best_only=True,\n        verbose=1\n    ),\n    embedding_saver\n]\n\nprint(\"✅ Callbacks defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:28:51.466133Z","iopub.execute_input":"2026-02-01T08:28:51.466388Z","iopub.status.idle":"2026-02-01T08:29:41.479626Z","shell.execute_reply.started":"2026-02-01T08:28:51.466359Z","shell.execute_reply":"2026-02-01T08:29:41.478950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Train the model with embedding saving\nprint(\"Starting training...\")\nprint(\"Note: Embeddings will be saved at each epoch for ROC-AUC stability analysis\")\nprint(\"This may increase training time slightly\\n\")\n\nEPOCHS = 20\n\n# FIXED: Use train_model instead of model, and prepared datasets\nhistory = train_model.fit(\n    train_ds_for_model,\n    validation_data=val_ds_for_model,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"✅ Training completed\")\nprint(\"✅ Embeddings saved for each epoch in 'embeddings_by_epoch' folder\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T08:29:41.480494Z","iopub.execute_input":"2026-02-01T08:29:41.480725Z","iopub.status.idle":"2026-02-01T10:22:46.441688Z","shell.execute_reply.started":"2026-02-01T08:29:41.480700Z","shell.execute_reply":"2026-02-01T10:22:46.440947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Save training history\nprint(\"Saving training history...\")\n\n# Convert history to serializable format\nhistory_dict = {}\nfor key, values in history.history.items():\n    if isinstance(values, list):\n        history_dict[key] = [float(v) for v in values]\n\n# Save to JSON\nwith open('training_history.json', 'w') as f:\n    json.dump(history_dict, f, indent=4)\n\n# Save to pickle\nwith open('training_history.pkl', 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(\"✅ Training history saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:22:46.442804Z","iopub.execute_input":"2026-02-01T10:22:46.443050Z","iopub.status.idle":"2026-02-01T10:22:46.450729Z","shell.execute_reply.started":"2026-02-01T10:22:46.443029Z","shell.execute_reply":"2026-02-01T10:22:46.450088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAVE_DIR = \"/kaggle/working/arcface_model\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Save complete model\ntrain_model.save(os.path.join(SAVE_DIR, \"arcface_resnet50_final.keras\"))\n\n# Save only the backbone for inference\nembedding_model = tf.keras.Model(\n    inputs=backbone.input,\n    outputs=backbone.output,\n    name=\"embedding_model\"\n)\nembedding_model.save(os.path.join(SAVE_DIR, \"embedding_model.keras\"))\n\n# Save class names\nwith open(os.path.join(SAVE_DIR, \"class_names.pkl\"), \"wb\") as f:\n    pickle.dump(all_class_names, f)\n\nprint(\"✅ Model saved successfully\")\nprint(\"Saved files:\")\nprint(f\"1. {SAVE_DIR}/arcface_resnet50_final.keras - Complete model\")\nprint(f\"2. {SAVE_DIR}/embedding_model.keras - Embedding model for inference\")\nprint(f\"3. {SAVE_DIR}/class_names.pkl - Class names\")\nprint(f\"4. {SAVE_DIR}/training_history.json - Training metrics\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:22:46.451775Z","iopub.execute_input":"2026-02-01T10:22:46.452024Z","iopub.status.idle":"2026-02-01T10:22:48.773592Z","shell.execute_reply.started":"2026-02-01T10:22:46.452004Z","shell.execute_reply":"2026-02-01T10:22:48.772818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Define evaluation functions for face verification\ndef extract_embeddings(model, dataset):\n    \"\"\"Extract embeddings from dataset\"\"\"\n    embeddings_list = []\n    labels_list = []\n    \n    print(\"Extracting embeddings...\")\n    for images, labels in tqdm(dataset, desc=\"Processing batches\"):\n        # Get only images (skip labels for inference)\n        embeddings = model.predict(images, verbose=0)\n        embeddings_list.append(embeddings)\n        labels_list.append(labels.numpy())\n    \n    return np.vstack(embeddings_list), np.concatenate(labels_list)\n\ndef evaluate_face_verification(gallery_embeddings, gallery_labels, \n                              probe_embeddings, probe_labels, threshold=0.5):\n    \"\"\"\n    Evaluate face verification performance using cosine similarity\n    \n    Args:\n        gallery_embeddings: Embeddings from training set (known identities)\n        gallery_labels: Labels for gallery set\n        probe_embeddings: Embeddings from validation set (query identities)\n        probe_labels: Labels for probe set\n        threshold: Similarity threshold for verification\n    \"\"\"\n    from sklearn.metrics.pairwise import cosine_similarity\n    \n    # Calculate similarity matrix between gallery and probe\n    print(\"Calculating similarity matrix...\")\n    similarity_matrix = cosine_similarity(probe_embeddings, gallery_embeddings)\n    \n    # For each probe image, find most similar gallery image\n    predictions = []\n    true_labels = []\n    \n    print(\"Matching probe to gallery...\")\n    for i in tqdm(range(len(probe_embeddings)), desc=\"Evaluating matches\"):\n        max_similarity_idx = np.argmax(similarity_matrix[i])\n        max_similarity = similarity_matrix[i, max_similarity_idx]\n        \n        # Predict same identity if similarity > threshold\n        predicted_same = max_similarity > threshold\n        \n        # For class-disjoint validation: All should be FALSE (different identities)\n        # But we check if it's the same label index (though identities are different)\n        actual_same = (probe_labels[i] == gallery_labels[max_similarity_idx])\n        \n        predictions.append(predicted_same)\n        true_labels.append(actual_same)\n    \n    # Calculate metrics\n    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions, zero_division=0)\n    recall = recall_score(true_labels, predictions, zero_division=0)\n    f1 = f1_score(true_labels, predictions, zero_division=0)\n    cm = confusion_matrix(true_labels, predictions)\n    \n    print(f\"\\nFace Verification Results (threshold={threshold}):\")\n    print(f\"  Accuracy: {accuracy:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall: {recall:.4f}\")\n    print(f\"  F1-Score: {f1:.4f}\")\n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  True Negatives: {cm[0,0]}\")\n    print(f\"  False Positives: {cm[0,1]}\")\n    print(f\"  False Negatives: {cm[1,0]}\")\n    print(f\"  True Positives: {cm[1,1]}\")\n    \n    # Calculate verification rate at different thresholds\n    print(\"\\nCalculating similarity distributions...\")\n    similarities = []\n    same_pairs = []\n    \n    # Sample pairs for efficiency (optional)\n    sample_size = min(10000, len(probe_embeddings) * len(gallery_embeddings))\n    indices = np.random.choice(len(probe_embeddings) * len(gallery_embeddings), \n                              sample_size, replace=False)\n    \n    for idx in tqdm(indices, desc=\"Sampling pairs\"):\n        i = idx // len(gallery_embeddings)\n        j = idx % len(gallery_embeddings)\n        \n        if i >= len(probe_embeddings):\n            continue\n            \n        sim = cosine_similarity([probe_embeddings[i]], [gallery_embeddings[j]])[0][0]\n        similarities.append(sim)\n        same_pairs.append(probe_labels[i] == gallery_labels[j])\n    \n    return accuracy, precision, recall, f1, similarities, same_pairs, cm\n\nprint(\"✅ Evaluation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:22:48.774705Z","iopub.execute_input":"2026-02-01T10:22:48.775432Z","iopub.status.idle":"2026-02-01T10:22:48.786821Z","shell.execute_reply.started":"2026-02-01T10:22:48.775407Z","shell.execute_reply":"2026-02-01T10:22:48.786192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Evaluate the trained model (memory-safe)\nimport gc\nprint(\"Evaluating face verification performance...\")\n\ndef extract_embeddings_safe(model, dataset):\n    \"\"\"Extract embeddings batch-by-batch without model.predict overhead\"\"\"\n    embeddings_list = []\n    labels_list = []\n    for images, labels in dataset:\n        emb = model(images, training=False).numpy()\n        embeddings_list.append(emb)\n        labels_list.append(labels.numpy())\n    return np.vstack(embeddings_list), np.concatenate(labels_list)\n\n\n# 1. Extract gallery embeddings (training set)\nprint(\"\\n1. Extracting gallery embeddings (training set)...\")\ngallery_embeddings, gallery_labels = extract_embeddings_safe(inference_model, train_ds)\n\n# 2. Extract probe / validation embeddings\nprint(\"\\n2. Extracting probe embeddings (validation set)...\")\nprobe_embeddings, probe_labels = extract_embeddings_safe(inference_model, val_ds)\n\nprint(f\"\\nDataset sizes:\")\nprint(f\"  Gallery: {len(gallery_embeddings)} embeddings, {len(np.unique(gallery_labels))} identities\")\nprint(f\"  Probe:   {len(probe_embeddings)} embeddings, {len(np.unique(probe_labels))} identities\")\n\n# --------------- chunked verification eval ---------------\nfrom sklearn.metrics.pairwise import cosine_similarity as sk_cosine\n\nprint(\"\\n3. Computing gallery<->probe similarity (chunked to save RAM)...\")\n\nCHUNK = 256\nn_probe   = len(probe_embeddings)\nn_gallery = len(gallery_embeddings)\n\nmax_sim_idx = np.empty(n_probe, dtype=np.int64)\nmax_sim_val = np.empty(n_probe, dtype=np.float32)\n\nfor start in range(0, n_probe, CHUNK):\n    end   = min(start + CHUNK, n_probe)\n    block = sk_cosine(probe_embeddings[start:end], gallery_embeddings)  # (chunk, gallery)\n    max_sim_idx[start:end] = block.argmax(axis=1)\n    max_sim_val[start:end] = block.max(axis=1)\n    del block   # free chunk immediately\n\nTHRESHOLD = 0.5\npredictions = max_sim_val > THRESHOLD\ntrue_labels = probe_labels == gallery_labels[max_sim_idx]\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix as sk_cm\n\naccuracy  = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions, zero_division=0)\nrecall    = recall_score(true_labels, predictions, zero_division=0)\nf1        = f1_score(true_labels, predictions, zero_division=0)\ncm        = sk_cm(true_labels, predictions)\n\nprint(f\"\\nFace Verification Results (threshold={THRESHOLD}):\")\nprint(f\"  Accuracy : {accuracy:.4f}\")\nprint(f\"  Precision: {precision:.4f}\")\nprint(f\"  Recall   : {recall:.4f}\")\nprint(f\"  F1-Score : {f1:.4f}\")\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"  TN={cm[0,0]}  FP={cm[0,1]}  FN={cm[1,0]}  TP={cm[1,1]}\")\n\n# Sample random pairs for similarity-distribution plots\nprint(\"\\nSampling pairs for similarity distributions...\")\nsample_size = min(10000, n_probe * n_gallery)\nsimilarities = []\nsame_pairs   = []\n\nfor _ in range(sample_size):\n    i = np.random.randint(n_probe)\n    j = np.random.randint(n_gallery)\n    # Embeddings are L2-normalised so dot product == cosine similarity\n    sim = float(probe_embeddings[i] @ gallery_embeddings[j])\n    similarities.append(sim)\n    same_pairs.append(int(probe_labels[i] == gallery_labels[j]))\n\nsimilarities = np.array(similarities, dtype=np.float32)\nsame_pairs   = np.array(same_pairs,   dtype=np.int8)\n\n# ---- FREE the large gallery array — no longer needed ----\ndel gallery_embeddings, gallery_labels, max_sim_idx, max_sim_val\ngc.collect()\n\n# Keep only what the later cells actually need\nval_embeddings = probe_embeddings\nval_labels     = probe_labels\n\nprint(\"\\n✅ Face verification evaluation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:22:48.787866Z","iopub.execute_input":"2026-02-01T10:22:48.788372Z","iopub.status.idle":"2026-02-01T10:33:34.702606Z","shell.execute_reply.started":"2026-02-01T10:22:48.788350Z","shell.execute_reply":"2026-02-01T10:33:34.701807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Visualization of results\nprint(\"Visualizing results...\")\n\n# Plot training history\nif 'history' in locals():\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot loss\n    axes[0].plot(history.history['loss'], label='Training Loss')\n    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')5.8GiB\nMax 29GiB\nGPU\nGPU\n0.00%\nGPU Memory\n12.3GiB\nMax 16GiB\n￼power_settings_new\n￼loop\n￼more_vert\n￼\n￼terminal￼keyboard\n￼\narrow_left\nSession started.\nRAM\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Plot accuracy\n    axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot similarity distributions\nif 'similarities' in locals() and 'same_pairs' in locals():\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Convert to numpy arrays\n    similarities = np.array(similarities)\n    same_pairs = np.array(same_pairs)\n    \n    # Plot histogram of similarities\n    axes[0].hist(similarities[same_pairs == 0], bins=50, alpha=0.7, \n                label='Different Identities', density=True)\n    axes[0].hist(similarities[same_pairs == 1], bins=50, alpha=0.7, \n                label='Same Identity', density=True)\n    axes[0].set_title('Similarity Distribution')\n    axes[0].set_xlabel('Cosine Similarity')\n    axes[0].set_ylabel('Density')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Plot ROC curve (if we have enough data)\n    if len(np.unique(same_pairs)) > 1:\n        from sklearn.metrics import roc_curve, auc\n        \n        fpr, tpr, thresholds = roc_curve(same_pairs, similarities)\n        roc_auc = auc(fpr, tpr)\n        \n        axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n                    label=f'ROC curve (AUC = {roc_auc:.3f})')\n        axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n        axes[1].set_xlim([0.0, 1.0])\n        axes[1].set_ylim([0.0, 1.05])\n        axes[1].set_xlabel('False Positive Rate')\n        axes[1].set_ylabel('True Positive Rate')\n        axes[1].set_title('Receiver Operating Characteristic')\n        axes[1].legend(loc=\"lower right\")\n        axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"✅ Visualizations complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:34.703591Z","iopub.execute_input":"2026-02-01T10:33:34.703853Z","iopub.status.idle":"2026-02-01T10:33:35.482609Z","shell.execute_reply.started":"2026-02-01T10:33:34.703833Z","shell.execute_reply":"2026-02-01T10:33:35.481869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15: Define basic evaluation functions (memory-safe)\ndef compute_similarity_scores(embeddings, labels, max_n=2000):\n    \"\"\"Compute similarity scores — subsample if too large to avoid N×N explosion\"\"\"\n    n = len(embeddings)\n    if n > max_n:\n        idx = np.random.choice(n, max_n, replace=False)\n        embeddings = embeddings[idx]\n        labels     = labels[idx]\n        n = max_n\n\n    similarity_matrix = sklearn_cosine_similarity(embeddings)\n\n    # Build pair labels using vectorised broadcasting\n    pair_labels = (labels[:, None] == labels[None, :]).astype(np.int8)\n\n    # Upper triangle only (no diagonal, no double-counting)\n    mask        = np.triu_indices(n, k=1)\n    scores      = similarity_matrix[mask].astype(np.float32)\n    labels_flat = pair_labels[mask]\n\n    del similarity_matrix, pair_labels  # free immediately\n\n    return scores, labels_flat\n\n\ndef get_predicted_labels(embeddings, labels):\n    \"\"\"Get predicted labels using nearest neighbour (chunked)\"\"\"\n    n     = len(embeddings)\n    preds = np.empty(n, dtype=labels.dtype)\n    CHUNK = 256\n\n    for start in range(0, n, CHUNK):\n        end = min(start + CHUNK, n)\n        sim = sklearn_cosine_similarity(embeddings[start:end], embeddings)\n        # Zero out self-similarity for items in this chunk\n        for local_i, global_i in enumerate(range(start, end)):\n            sim[local_i, global_i] = -1.0\n        preds[start:end] = labels[sim.argmax(axis=1)]\n        del sim  # free each chunk\n    return preds\n\nprint(\"✅ Basic evaluation functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:35.484051Z","iopub.execute_input":"2026-02-01T10:33:35.484354Z","iopub.status.idle":"2026-02-01T10:33:35.492309Z","shell.execute_reply.started":"2026-02-01T10:33:35.484330Z","shell.execute_reply":"2026-02-01T10:33:35.491549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 16: Plot training history\nprint(\"Plotting training history...\")\n\n# Extract history\nhistory_data = history.history\nepochs = range(1, len(history_data['accuracy']) + 1)\n\n# Create figure with subplots\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# Plot 1: Training & Validation Accuracy\naxes[0, 0].plot(epochs, history_data['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\naxes[0, 0].plot(epochs, history_data['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\naxes[0, 0].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\naxes[0, 0].set_xlabel('Epochs', fontsize=12)\naxes[0, 0].set_ylabel('Accuracy', fontsize=12)\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Training & Validation Loss\naxes[0, 1].plot(epochs, history_data['loss'], 'b-', linewidth=2, label='Training Loss')\naxes[0, 1].plot(epochs, history_data['val_loss'], 'r-', linewidth=2, label='Validation Loss')\naxes[0, 1].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0, 1].set_xlabel('Epochs', fontsize=12)\naxes[0, 1].set_ylabel('Loss', fontsize=12)\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Learning Rate Schedule (if available)\nif 'lr' in history_data:\n    axes[1, 0].plot(epochs, history_data['lr'], 'g-', linewidth=2)\n    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epochs', fontsize=12)\n    axes[1, 0].set_ylabel('Learning Rate', fontsize=12)\n    axes[1, 0].set_yscale('log')\n    axes[1, 0].grid(True, alpha=0.3)\n\n# Plot 4: Accuracy vs Loss\nax2 = axes[1, 1] if 'lr' in history_data else axes[1, 0]\nax2.scatter(history_data['loss'], history_data['accuracy'], alpha=0.6, \n            c='blue', label='Training', s=50)\nax2.scatter(history_data['val_loss'], history_data['val_accuracy'], alpha=0.6, \n            c='red', label='Validation', s=50)\nax2.set_title('Accuracy vs Loss', fontsize=14, fontweight='bold')\nax2.set_xlabel('Loss', fontsize=12)\nax2.set_ylabel('Accuracy', fontsize=12)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_history.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✅ Training history plots saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:35.493301Z","iopub.execute_input":"2026-02-01T10:33:35.493539Z","iopub.status.idle":"2026-02-01T10:33:37.763396Z","shell.execute_reply.started":"2026-02-01T10:33:35.493520Z","shell.execute_reply":"2026-02-01T10:33:37.762675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 17: Compute basic metrics\nprint(\"Computing basic metrics...\")\n\n# Check if val_embeddings exists, if not extract them\nif 'val_embeddings' not in locals() or 'val_labels' not in locals():\n    print(\"Extracting validation embeddings...\")\n    val_embeddings, val_labels = extract_embeddings(inference_model, val_ds)\n\n# Compute similarity scores for verification\nscores, pair_labels = compute_similarity_scores(val_embeddings, val_labels)\n\n# Compute ROC-AUC\nfpr, tpr, thresholds = roc_curve(pair_labels, scores)\nroc_auc = auc(fpr, tpr)\n\n# Compute Top-1 accuracy\npreds = get_predicted_labels(val_embeddings, val_labels)\ntop1_accuracy = accuracy_score(val_labels, preds)\n\nprint(f\"✅ ROC-AUC: {roc_auc:.4f}\")\nprint(f\"✅ Top-1 Accuracy: {top1_accuracy:.4f}\")\n\n# Save basic metrics\nbasic_metrics = {\n    'roc_auc': float(roc_auc),\n    'top1_accuracy': float(top1_accuracy),\n    'fpr': fpr.tolist(),\n    'tpr': tpr.tolist(),\n    'thresholds': thresholds.tolist()\n}\n\nwith open('basic_metrics.json', 'w') as f:\n    json.dump(basic_metrics, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:37.764257Z","iopub.execute_input":"2026-02-01T10:33:37.764521Z","iopub.status.idle":"2026-02-01T10:33:53.171597Z","shell.execute_reply.started":"2026-02-01T10:33:37.764500Z","shell.execute_reply":"2026-02-01T10:33:53.170998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 18: Plot ROC Curve\nprint(\"Plotting ROC Curve...\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label=f'ROC curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n         label='Random Classifier')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)', fontsize=12)\nplt.ylabel('True Positive Rate (TPR)', fontsize=12)\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\n\n# Add some important points\nfor far_target in [0.001, 0.01, 0.1]:\n    idx = np.argmin(np.abs(fpr - far_target))\n    if idx < len(tpr):\n        plt.scatter(fpr[idx], tpr[idx], s=100, zorder=5, \n                   label=f'FAR={far_target}: TAR={tpr[idx]:.3f}')\n        plt.annotate(f'TAR={tpr[idx]:.3f}', \n                    (fpr[idx], tpr[idx]),\n                    xytext=(10, 10), textcoords='offset points')\n\nplt.legend(loc=\"lower right\", fontsize=10)\nplt.tight_layout()\nplt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✅ ROC Curve saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:53.172540Z","iopub.execute_input":"2026-02-01T10:33:53.172857Z","iopub.status.idle":"2026-02-01T10:33:54.157736Z","shell.execute_reply.started":"2026-02-01T10:33:53.172835Z","shell.execute_reply":"2026-02-01T10:33:54.156849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 19: Plot Precision-Recall Curve\nprint(\"Plotting Precision-Recall Curve...\")\n\nprecision, recall, pr_thresholds = precision_recall_curve(pair_labels, scores)\navg_precision = average_precision_score(pair_labels, scores)\n\nplt.figure(figsize=(10, 8))\nplt.plot(recall, precision, color='darkgreen', lw=2, \n         label=f'AP = {avg_precision:.4f}')\nplt.fill_between(recall, precision, alpha=0.2, color='green')\nplt.xlabel('Recall', fontsize=12)\nplt.ylabel('Precision', fontsize=12)\nplt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.legend(loc=\"lower left\")\nplt.grid(True, alpha=0.3)\n\n# Add F1-score contours\nf1_scores = np.linspace(0.1, 0.9, 9)\nfor f1 in f1_scores:\n    if f1 == 0:\n        continue\n    x = np.linspace(0.01, 1)\n    y = f1 * x / (2 * x - f1)\n    plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2, linestyle='--')\n    plt.text(0.9, y[-1] - 0.05, f'F1={f1:.1f}', fontsize=8, alpha=0.5)\n\nplt.tight_layout()\nplt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✅ Precision-Recall Curve saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:33:54.158973Z","iopub.execute_input":"2026-02-01T10:33:54.159270Z","iopub.status.idle":"2026-02-01T10:34:00.613565Z","shell.execute_reply.started":"2026-02-01T10:33:54.159238Z","shell.execute_reply":"2026-02-01T10:34:00.612924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 20: Plot TAR vs FAR Curve\nprint(\"Plotting TAR vs FAR Curve...\")\n\n# Sort thresholds and corresponding TPR, FPR\nsorted_idx = np.argsort(thresholds)[::-1]\nsorted_fpr = fpr[sorted_idx]\nsorted_tpr = tpr[sorted_idx]\n\nplt.figure(figsize=(10, 8))\nplt.semilogx(sorted_fpr, sorted_tpr, 'b-', linewidth=2)\nplt.xlim([1e-4, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Acceptance Rate (FAR)', fontsize=12)\nplt.ylabel('True Acceptance Rate (TAR)', fontsize=12)\nplt.title('TAR vs FAR Curve (Verification Performance)', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, which='both')\n\n# Mark specific operating points\nfar_targets = [1e-4, 1e-3, 1e-2, 1e-1]\nfor far in far_targets:\n    idx = np.argmin(np.abs(sorted_fpr - far))\n    if idx < len(sorted_tpr):\n        plt.scatter(sorted_fpr[idx], sorted_tpr[idx], s=100, color='red', zorder=5)\n        plt.annotate(f'FAR={far:.0e}\\nTAR={sorted_tpr[idx]:.3f}', \n                    (sorted_fpr[idx], sorted_tpr[idx]),\n                    xytext=(10, 10), textcoords='offset points',\n                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.5))\n\nplt.tight_layout()\nplt.savefig('tar_vs_far_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint(\"✅ TAR vs FAR Curve saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:00.614489Z","iopub.execute_input":"2026-02-01T10:34:00.614726Z","iopub.status.idle":"2026-02-01T10:34:01.754242Z","shell.execute_reply.started":"2026-02-01T10:34:00.614706Z","shell.execute_reply":"2026-02-01T10:34:01.753520Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 21: Compute and plot Per-Class Accuracy Distribution\nprint(\"Computing per-class accuracy...\")\n\n# Compute confusion matrix\ncm = confusion_matrix(val_labels, preds)\n\n# Calculate per-class accuracy\nper_class_accuracy = cm.diagonal() / cm.sum(axis=1)\nper_class_accuracy = np.nan_to_num(per_class_accuracy)  # Handle division by zero\n\n# Sort classes by accuracy\nsorted_idx = np.argsort(per_class_accuracy)[::-1]\nsorted_acc = per_class_accuracy[sorted_idx]\n\n# Plot distribution\nplt.figure(figsize=(14, 6))\n\n# Plot 1: Histogram of accuracies\nplt.subplot(1, 2, 1)\nplt.hist(per_class_accuracy, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\nplt.xlabel('Per-Class Accuracy', fontsize=12)\nplt.ylabel('Number of Classes', fontsize=12)\nplt.title('Distribution of Per-Class Accuracies', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\n\n# Add statistics\nmean_acc = np.mean(per_class_accuracy)\nmedian_acc = np.median(per_class_accuracy)\nstd_acc = np.std(per_class_accuracy)\nplt.axvline(mean_acc, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_acc:.3f}')\nplt.axvline(median_acc, color='green', linestyle='--', linewidth=2, label=f'Median: {median_acc:.3f}')\nplt.legend()\n\n# Plot 2: Sorted per-class accuracy\nplt.subplot(1, 2, 2)\nplt.plot(range(len(sorted_acc)), sorted_acc, 'b-', linewidth=1, alpha=0.7)\nplt.fill_between(range(len(sorted_acc)), sorted_acc, alpha=0.3, color='blue')\nplt.xlabel('Class Rank (Sorted by Accuracy)', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.title('Sorted Per-Class Accuracy', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3)\n\n# Mark top 20 and bottom 20\ntop_20_idx = sorted_idx[:20]\nbottom_20_idx = sorted_idx[-20:]\nplt.scatter(range(20), sorted_acc[:20], color='green', s=50, label='Top 20 Classes', zorder=5)\nplt.scatter(range(len(sorted_acc)-20, len(sorted_acc)), sorted_acc[-20:], \n            color='red', s=50, label='Bottom 20 Classes', zorder=5)\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('per_class_accuracy_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✅ Per-class accuracy computed\")\nprint(f\"   Mean: {mean_acc:.4f}\")\nprint(f\"   Median: {median_acc:.4f}\")\nprint(f\"   Std: {std_acc:.4f}\")\nprint(f\"   Min: {np.min(per_class_accuracy):.4f}\")\nprint(f\"   Max: {np.max(per_class_accuracy):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:01.757174Z","iopub.execute_input":"2026-02-01T10:34:01.757451Z","iopub.status.idle":"2026-02-01T10:34:02.982774Z","shell.execute_reply.started":"2026-02-01T10:34:01.757430Z","shell.execute_reply":"2026-02-01T10:34:02.982070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 22: Quick Similarity Distribution\npos_scores = scores[pair_labels == 1]\nneg_scores = scores[pair_labels == 0]\n\nplt.figure(figsize=(7,5))\nplt.hist(pos_scores, bins=100, alpha=0.6, label=\"Intra-class (Genuine)\")\nplt.hist(neg_scores, bins=100, alpha=0.6, label=\"Inter-class (Impostor)\")\nplt.xlabel(\"Cosine Similarity\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Intra vs Inter Class Similarity Distribution\")\nplt.legend()\nplt.grid()\nplt.show()\n\nprint(\"Mean Intra-class similarity:\", np.mean(pos_scores))\nprint(\"Mean Inter-class similarity:\", np.mean(neg_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:02.983696Z","iopub.execute_input":"2026-02-01T10:34:02.983928Z","iopub.status.idle":"2026-02-01T10:34:03.362762Z","shell.execute_reply.started":"2026-02-01T10:34:02.983909Z","shell.execute_reply":"2026-02-01T10:34:03.362180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 23: Intra-Class vs Inter-Class Similarity Distribution (memory-safe)\nimport gc\nprint(\"Computing intra-class and inter-class similarities...\")\n\n# Subsample to at most 2000 samples so the N×N matrix stays small\n_n   = len(val_embeddings)\n_max = 2000\nif _n > _max:\n    _idx = np.random.choice(_n, _max, replace=False)\n    _emb = val_embeddings[_idx]\n    _lab = val_labels[_idx]\nelse:\n    _emb = val_embeddings\n    _lab = val_labels\n\n# Single N×N similarity matrix (N <= 2000 -> ~32 MB max)\n_sim = sklearn_cosine_similarity(_emb)\n\n# Upper-triangle mask (no diagonal, no double-counting)\n_i_idx, _j_idx = np.triu_indices(len(_lab), k=1)\n_scores  = _sim[_i_idx, _j_idx].astype(np.float32)\n_same    = (_lab[_i_idx] == _lab[_j_idx])\n\ndel _sim, _i_idx, _j_idx  # free the big matrix immediately\ngc.collect()\n\nintra_similarities = _scores[ _same].copy()\ninter_similarities = _scores[~_same].copy()\ndel _scores, _same, _emb, _lab\ngc.collect()\n\n# Cap at 10000 for plotting\nif len(intra_similarities) > 10000:\n    intra_similarities = np.random.choice(intra_similarities, 10000, replace=False)\nif len(inter_similarities) > 10000:\n    inter_similarities = np.random.choice(inter_similarities, 10000, replace=False)\n\n# ---------- plots ----------\nplt.figure(figsize=(14, 10))\n\nplt.subplot(2, 2, 1)\nplt.hist(intra_similarities, bins=100, alpha=0.6, color='blue',\n         label=f'Intra-Class (n={len(intra_similarities):,})', density=True)\nplt.hist(inter_similarities, bins=100, alpha=0.6, color='red',\n         label=f'Inter-Class (n={len(inter_similarities):,})', density=True)\nplt.xlabel('Cosine Similarity', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.title('Intra-Class vs Inter-Class Similarity Distribution', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(2, 2, 2)\nbox_data   = [intra_similarities, inter_similarities]\nbox_labels = ['Intra-Class', 'Inter-Class']\nbp = plt.boxplot(box_data, labels=box_labels, patch_artist=True)\nbp['boxes'][0].set_facecolor('lightblue')\nbp['boxes'][1].set_facecolor('lightcoral')\nplt.ylabel('Cosine Similarity', fontsize=12)\nplt.title('Similarity Distribution Comparison', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.subplot(2, 2, 3)\nparts = plt.violinplot(box_data, showmeans=True, showmedians=True)\nfor i, pc in enumerate(parts['bodies']):\n    pc.set_facecolor(['lightblue', 'lightcoral'][i])\n    pc.set_alpha(0.7)\nplt.xticks([1, 2], box_labels)\nplt.ylabel('Cosine Similarity', fontsize=12)\nplt.title('Violin Plot of Similarities', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, axis='y')\n\nplt.subplot(2, 2, 4)\nsorted_intra = np.sort(intra_similarities)\nsorted_inter = np.sort(inter_similarities)\ny_intra = np.arange(1, len(sorted_intra)+1) / len(sorted_intra)\ny_inter = np.arange(1, len(sorted_inter)+1) / len(sorted_inter)\nplt.plot(sorted_intra, y_intra, 'b-', label='Intra-Class', linewidth=2)\nplt.plot(sorted_inter, y_inter, 'r-', label='Inter-Class', linewidth=2)\nplt.xlabel('Cosine Similarity', fontsize=12)\nplt.ylabel('Cumulative Probability', fontsize=12)\nplt.title('Cumulative Distribution Function', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('intra_inter_class_similarity.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nintra_mean = np.mean(intra_similarities)\ninter_mean = np.mean(inter_similarities)\nintra_std  = np.std(intra_similarities)\ninter_std  = np.std(inter_similarities)\n\nprint(f\"✅ Intra-class similarity: Mean = {intra_mean:.4f}, Std = {intra_std:.4f}\")\nprint(f\"✅ Inter-class similarity: Mean = {inter_mean:.4f}, Std = {inter_std:.4f}\")\nprint(f\"✅ Separation: {intra_mean - inter_mean:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:03.363609Z","iopub.execute_input":"2026-02-01T10:34:03.363922Z","iopub.status.idle":"2026-02-01T10:34:06.580793Z","shell.execute_reply.started":"2026-02-01T10:34:03.363902Z","shell.execute_reply":"2026-02-01T10:34:06.580159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 24: Confusion Matrices for Top 20 and Bottom 20 Classes\nprint(\"Plotting confusion matrices...\")\n\n# Get accuracy per class\nclass_accuracies = per_class_accuracy\n\n# Get indices of top 20 and bottom 20 classes\ntop_20_idx = np.argsort(class_accuracies)[-20:][::-1]  # Highest to lowest\nbottom_20_idx = np.argsort(class_accuracies)[:20]  # Lowest to highest\n\n# Create confusion matrices for selected classes\ndef plot_confusion_matrix_for_classes(class_indices, title, filename):\n    \"\"\"Plot confusion matrix for specific classes\"\"\"\n    # Filter predictions and labels for selected classes\n    mask = np.isin(val_labels, class_indices)\n    filtered_labels = val_labels[mask]\n    filtered_preds = preds[mask]\n    \n    # Map to new indices for better visualization\n    idx_map = {old: new for new, old in enumerate(class_indices)}\n    mapped_labels = np.array([idx_map[l] for l in filtered_labels])\n    mapped_preds = np.array([idx_map[p] if p in idx_map else -1 for p in filtered_preds])\n    \n    # Create confusion matrix\n    cm = confusion_matrix(mapped_labels, mapped_preds, labels=range(len(class_indices)))\n    \n    # Plot\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=[f'C{idx}' for idx in class_indices],\n                yticklabels=[f'C{idx}' for idx in class_indices])\n    plt.xlabel('Predicted Label', fontsize=12)\n    plt.ylabel('True Label', fontsize=12)\n    plt.title(title, fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Plot top 20 classes\nplot_confusion_matrix_for_classes(\n    top_20_idx, \n    'Confusion Matrix - Top 20 Performing Classes',\n    'confusion_matrix_top20.png'\n)\n\n# Plot bottom 20 classes\nplot_confusion_matrix_for_classes(\n    bottom_20_idx,\n    'Confusion Matrix - Bottom 20 Performing Classes',\n    'confusion_matrix_bottom20.png'\n)\n\nprint(\"✅ Confusion matrices saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:06.581755Z","iopub.execute_input":"2026-02-01T10:34:06.581985Z","iopub.status.idle":"2026-02-01T10:34:10.525458Z","shell.execute_reply.started":"2026-02-01T10:34:06.581965Z","shell.execute_reply":"2026-02-01T10:34:10.524759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 25: Calibration Curve (Reliability Diagram)\nprint(\"Plotting calibration curve...\")\n\n# For multi-class calibration, we'll use the max softmax probability as confidence\n# First, get logits from the model\nall_logits = []\nall_labels = []\n\nfor images, labels in tqdm(val_ds, desc=\"Getting logits\"):\n    embeddings = backbone(images, training=False)\n    logits = arcface(embeddings)  # Use arcface without labels for inference\n    all_logits.append(logits.numpy())\n    all_labels.append(labels.numpy())\n\nall_logits = np.vstack(all_logits)\nall_labels = np.concatenate(all_labels)\n\n# Convert to probabilities using softmax\nprobabilities = tf.nn.softmax(all_logits).numpy()\n\n# Get predicted class and confidence\npredicted_class = np.argmax(probabilities, axis=1)\nconfidence = np.max(probabilities, axis=1)\ncorrect = (predicted_class == all_labels).astype(int)\n\n# Create calibration curve\nprob_true, prob_pred = calibration_curve(correct, confidence, n_bins=10, strategy='uniform')\n\nplt.figure(figsize=(10, 8))\nplt.plot(prob_pred, prob_true, 's-', linewidth=2, markersize=8, label='Model')\nplt.plot([0, 1], [0, 1], 'k--', label='Perfectly Calibrated')\n\n# Add bars for number of samples in each bin\nbin_counts = np.histogram(confidence, bins=10)[0]\nbin_centers = (prob_pred[1:] + prob_pred[:-1]) / 2\nplt.bar(bin_centers, prob_pred[:-1], width=0.08, alpha=0.3, \n        color='gray', label='Bin Density')\n\nplt.xlabel('Mean Predicted Probability', fontsize=12)\nplt.ylabel('Fraction of Positives', fontsize=12)\nplt.title('Reliability Diagram (Calibration Curve)', fontsize=14, fontweight='bold')\nplt.legend(loc='upper left')\nplt.grid(True, alpha=0.3)\n\n# Calculate ECE (Expected Calibration Error)\nbin_edges = np.linspace(0, 1, 11)\nbin_indices = np.digitize(confidence, bin_edges) - 1\nece = 0\nfor i in range(10):\n    mask = bin_indices == i\n    if np.sum(mask) > 0:\n        bin_conf = np.mean(confidence[mask])\n        bin_acc = np.mean(correct[mask])\n        ece += np.abs(bin_conf - bin_acc) * np.sum(mask)\nece /= len(confidence)\n\nplt.text(0.05, 0.95, f'ECE = {ece:.4f}', transform=plt.gca().transAxes,\n         fontsize=12, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.savefig('calibration_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✅ Calibration curve saved (ECE = {ece:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:34:10.526335Z","iopub.execute_input":"2026-02-01T10:34:10.526556Z","iopub.status.idle":"2026-02-01T10:36:04.719911Z","shell.execute_reply.started":"2026-02-01T10:34:10.526536Z","shell.execute_reply":"2026-02-01T10:36:04.719201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 26: Confidence Distribution (Correct vs Incorrect)\nprint(\"Plotting confidence distribution...\")\n\n# Separate confidence for correct and incorrect predictions\ncorrect_conf = confidence[correct == 1]\nincorrect_conf = confidence[correct == 0]\n\nplt.figure(figsize=(14, 6))\n\n# Plot 1: Histogram\nplt.subplot(1, 2, 1)\nplt.hist(correct_conf, bins=50, alpha=0.6, color='green', \n         label=f'Correct (n={len(correct_conf):,})', density=True)\nplt.hist(incorrect_conf, bins=50, alpha=0.6, color='red', \n         label=f'Incorrect (n={len(incorrect_conf):,})', density=True)\nplt.xlabel('Confidence', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.title('Confidence Distribution: Correct vs Incorrect', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Box plot\nplt.subplot(1, 2, 2)\nbox_data = [correct_conf, incorrect_conf]\nbox_labels = ['Correct', 'Incorrect']\nbp = plt.boxplot(box_data, labels=box_labels, patch_artist=True)\nbp['boxes'][0].set_facecolor('lightgreen')\nbp['boxes'][1].set_facecolor('lightcoral')\nplt.ylabel('Confidence', fontsize=12)\nplt.title('Confidence Comparison', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, axis='y')\n\n# Add statistics\nmean_correct = np.mean(correct_conf)\nmean_incorrect = np.mean(incorrect_conf)\nstd_correct = np.std(correct_conf)\nstd_incorrect = np.std(incorrect_conf)\n\nplt.text(0.05, 0.95, f'Correct: μ={mean_correct:.3f}, σ={std_correct:.3f}\\n'\n                     f'Incorrect: μ={mean_incorrect:.3f}, σ={std_incorrect:.3f}',\n         transform=plt.gca().transAxes, fontsize=10,\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.savefig('confidence_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✅ Confidence distribution saved\")\nprint(f\"   Correct predictions: Mean = {mean_correct:.4f}, Std = {std_correct:.4f}\")\nprint(f\"   Incorrect predictions: Mean = {mean_incorrect:.4f}, Std = {std_incorrect:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:36:04.720833Z","iopub.execute_input":"2026-02-01T10:36:04.721042Z","iopub.status.idle":"2026-02-01T10:36:05.819333Z","shell.execute_reply.started":"2026-02-01T10:36:04.721022Z","shell.execute_reply":"2026-02-01T10:36:05.818610Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 27: Confidence vs Accuracy Correlation\nprint(\"Plotting confidence vs accuracy correlation...\")\n\n# Bin confidence and compute accuracy per bin\nnum_bins = 20\nbins = np.linspace(0, 1, num_bins + 1)\nbin_centers = (bins[:-1] + bins[1:]) / 2\n\nbin_accuracies = []\nbin_confidences = []\nbin_counts = []\n\nfor i in range(num_bins):\n    mask = (confidence >= bins[i]) & (confidence < bins[i+1])\n    if np.sum(mask) > 0:\n        bin_acc = np.mean(correct[mask])\n        bin_conf = np.mean(confidence[mask])\n        bin_accuracies.append(bin_acc)\n        bin_confidences.append(bin_conf)\n        bin_counts.append(np.sum(mask))\n\nbin_accuracies = np.array(bin_accuracies)\nbin_confidences = np.array(bin_confidences)\nbin_counts = np.array(bin_counts)\n\nplt.figure(figsize=(12, 8))\n\n# Plot 1: Scatter plot with size proportional to bin count\nscatter = plt.scatter(bin_confidences, bin_accuracies, \n                      s=bin_counts/10, alpha=0.7, \n                      c=bin_counts, cmap='viridis')\nplt.colorbar(scatter, label='Number of Samples (scaled)')\nplt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n\n# Add regression line\nif len(bin_confidences) > 1:\n    z = np.polyfit(bin_confidences, bin_accuracies, 1)\n    p = np.poly1d(z)\n    plt.plot(bin_confidences, p(bin_confidences), \"r--\", alpha=0.8, \n             label=f'Linear fit: y={z[0]:.3f}x+{z[1]:.3f}')\n\nplt.xlabel('Mean Confidence in Bin', fontsize=12)\nplt.ylabel('Accuracy in Bin', fontsize=12)\nplt.title('Confidence vs Accuracy Correlation', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Calculate correlation\ncorrelation = np.corrcoef(bin_confidences, bin_accuracies)[0, 1]\nplt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', transform=plt.gca().transAxes,\n         fontsize=12, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.savefig('confidence_vs_accuracy.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✅ Confidence vs Accuracy plot saved (Correlation: {correlation:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:36:05.820414Z","iopub.execute_input":"2026-02-01T10:36:05.820808Z","iopub.status.idle":"2026-02-01T10:36:06.754787Z","shell.execute_reply.started":"2026-02-01T10:36:05.820787Z","shell.execute_reply":"2026-02-01T10:36:06.754181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 28: ROC-AUC Stability Across Training (Real Analysis)\nprint(\"Analyzing ROC-AUC stability across training...\")\n\n# Helper function for fallback analysis\ndef analyze_from_saved_embeddings(embeddings_dir, val_labels):\n    \"\"\"Analyze ROC-AUC from saved embedding files\"\"\"\n    epoch_files = sorted([f for f in os.listdir(embeddings_dir) if f.startswith('embeddings_epoch_')])\n    epoch_roc_aucs = []\n    \n    for i, epoch_file in enumerate(tqdm(epoch_files, desc=\"Analyzing saved embeddings\")):\n        embeddings = np.load(os.path.join(embeddings_dir, epoch_file))\n        scores, pair_labels = compute_similarity_scores_for_auc(embeddings, val_labels)\n        roc_auc = roc_auc_score(pair_labels, scores)\n        epoch_roc_aucs.append(roc_auc)\n    \n    # Save the computed ROC-AUC values\n    np.save(os.path.join(embeddings_dir, 'roc_auc_history.npy'), epoch_roc_aucs)\n    \n    return epoch_roc_aucs\n\ndef create_demonstration_plot(history):\n    \"\"\"Create demonstration plot when no embeddings are saved\"\"\"\n    epochs = range(1, len(history.history['val_accuracy']) + 1)\n    # Create realistic-looking synthetic data\n    roc_auc_values = 0.7 + 0.3 * (1 - np.exp(-np.array(epochs) / 8)) + np.random.normal(0, 0.01, len(epochs))\n    \n    plt.figure(figsize=(10, 8))\n    plt.plot(epochs, roc_auc_values, 'b-o', linewidth=2, markersize=8)\n    plt.fill_between(epochs, roc_auc_values - 0.02, roc_auc_values + 0.02, alpha=0.2, color='blue')\n    \n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('ROC-AUC', fontsize=12)\n    plt.title('ROC-AUC Stability Across Training (Demonstration)', fontsize=14, fontweight='bold')\n    plt.grid(True, alpha=0.3)\n    \n    best_epoch = np.argmax(roc_auc_values)\n    plt.scatter(epochs[best_epoch], roc_auc_values[best_epoch], \n               color='red', s=200, zorder=5, \n               label=f'Best: {roc_auc_values[best_epoch]:.4f} @ Epoch {epochs[best_epoch]}')\n    \n    plt.legend()\n    plt.tight_layout()\n    plt.savefig('roc_auc_stability_demo.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"⚠️ Created demonstration plot (not actual data)\")\n    print(\"   For real analysis, ensure embeddings are saved during training\")\n\ndef create_stability_plot(epochs, roc_auc_values):\n    \"\"\"Create stability plot from computed values\"\"\"\n    plt.figure(figsize=(10, 8))\n    plt.plot(epochs, roc_auc_values, 'b-o', linewidth=2, markersize=8)\n    \n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('ROC-AUC', fontsize=12)\n    plt.title('ROC-AUC Stability Across Training', fontsize=14, fontweight='bold')\n    plt.grid(True, alpha=0.3)\n    \n    best_epoch = np.argmax(roc_auc_values)\n    plt.scatter(epochs[best_epoch], roc_auc_values[best_epoch], \n               color='red', s=200, zorder=5, \n               label=f'Best: {roc_auc_values[best_epoch]:.4f} @ Epoch {epochs[best_epoch]}')\n    \n    plt.legend()\n    plt.tight_layout()\n    plt.savefig('roc_auc_stability.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Load ROC-AUC history from saved files\nembeddings_dir = 'embeddings_by_epoch'\n\n# Check if embeddings were saved\nif os.path.exists(embeddings_dir):\n    # Load ROC-AUC history if available\n    roc_auc_file = os.path.join(embeddings_dir, 'roc_auc_history.npy')\n    if os.path.exists(roc_auc_file):\n        epoch_roc_aucs = np.load(roc_auc_file)\n        epochs = range(1, len(epoch_roc_aucs) + 1)\n        \n        print(f\"Found ROC-AUC values for {len(epoch_roc_aucs)} epochs\")\n        \n        # Load validation accuracy for comparison\n        val_accuracies = history.history.get('val_accuracy', [])\n        val_losses = history.history.get('val_loss', [])\n        \n        # Create comprehensive stability analysis\n        plt.figure(figsize=(16, 12))\n        \n        # Plot 1: ROC-AUC vs Epoch\n        plt.subplot(2, 2, 1)\n        plt.plot(epochs, epoch_roc_aucs, 'b-o', linewidth=2, markersize=8, label='ROC-AUC')\n        \n        # Add smoothing\n        if len(epoch_roc_aucs) > 5:\n            from scipy.ndimage import gaussian_filter1d\n            smoothed = gaussian_filter1d(epoch_roc_aucs, sigma=1)\n            plt.plot(epochs, smoothed, 'r--', linewidth=2, alpha=0.7, label='Smoothed')\n        \n        plt.xlabel('Epoch', fontsize=12)\n        plt.ylabel('ROC-AUC', fontsize=12)\n        plt.title('ROC-AUC Stability Across Training', fontsize=14, fontweight='bold')\n        plt.grid(True, alpha=0.3)\n        \n        # Mark best epoch\n        best_epoch_idx = np.argmax(epoch_roc_aucs)\n        best_roc_auc = epoch_roc_aucs[best_epoch_idx]\n        plt.scatter(epochs[best_epoch_idx], best_roc_auc, \n                   color='red', s=200, zorder=5, \n                   label=f'Best: {best_roc_auc:.4f} @ Epoch {epochs[best_epoch_idx]}')\n        \n        # Add final epoch\n        final_roc_auc = epoch_roc_aucs[-1]\n        plt.scatter(epochs[-1], final_roc_auc, \n                   color='green', s=200, zorder=5,\n                   label=f'Final: {final_roc_auc:.4f}')\n        \n        plt.legend()\n        plt.ylim([max(0.5, min(epoch_roc_aucs) - 0.05), min(1.0, max(epoch_roc_aucs) + 0.05)])\n        \n        # Plot 2: ROC-AUC vs Validation Accuracy\n        plt.subplot(2, 2, 2)\n        if len(val_accuracies) == len(epoch_roc_aucs):\n            plt.scatter(val_accuracies, epoch_roc_aucs, c=epochs, cmap='viridis', s=100)\n            plt.colorbar(label='Epoch')\n            \n            # Add correlation line\n            z = np.polyfit(val_accuracies, epoch_roc_aucs, 1)\n            p = np.poly1d(z)\n            plt.plot(val_accuracies, p(val_accuracies), \"r--\", alpha=0.8, \n                    label=f'Correlation: {np.corrcoef(val_accuracies, epoch_roc_aucs)[0,1]:.3f}')\n            \n            plt.xlabel('Validation Accuracy', fontsize=12)\n            plt.ylabel('ROC-AUC', fontsize=12)\n            plt.title('ROC-AUC vs Validation Accuracy', fontsize=14, fontweight='bold')\n            plt.grid(True, alpha=0.3)\n            plt.legend()\n        \n        # Plot 3: ROC-AUC vs Validation Loss\n        plt.subplot(2, 2, 3)\n        if len(val_losses) == len(epoch_roc_aucs):\n            plt.scatter(val_losses, epoch_roc_aucs, c=epochs, cmap='plasma', s=100)\n            plt.colorbar(label='Epoch')\n            \n            # Add correlation line\n            z = np.polyfit(val_losses, epoch_roc_aucs, 1)\n            p = np.poly1d(z)\n            plt.plot(val_losses, p(val_losses), \"r--\", alpha=0.8,\n                    label=f'Correlation: {np.corrcoef(val_losses, epoch_roc_aucs)[0,1]:.3f}')\n            \n            plt.xlabel('Validation Loss', fontsize=12)\n            plt.ylabel('ROC-AUC', fontsize=12)\n            plt.title('ROC-AUC vs Validation Loss', fontsize=14, fontweight='bold')\n            plt.grid(True, alpha=0.3)\n            plt.legend()\n        \n        # Plot 4: ROC-AUC Improvement per Epoch\n        plt.subplot(2, 2, 4)\n        if len(epoch_roc_aucs) > 1:\n            improvements = np.diff(epoch_roc_aucs)\n            plt.bar(epochs[1:], improvements, color=['green' if x > 0 else 'red' for x in improvements])\n            plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n            plt.xlabel('Epoch', fontsize=12)\n            plt.ylabel('ROC-AUC Improvement', fontsize=12)\n            plt.title('ROC-AUC Improvement per Epoch', fontsize=14, fontweight='bold')\n            plt.grid(True, alpha=0.3, axis='y')\n            \n            # Add cumulative improvement\n            cumulative_improvement = epoch_roc_aucs[-1] - epoch_roc_aucs[0]\n            plt.text(0.05, 0.95, f'Total Improvement: {cumulative_improvement:.4f}',\n                    transform=plt.gca().transAxes, fontsize=12,\n                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n        \n        plt.tight_layout()\n        plt.savefig('roc_auc_stability_analysis.png', dpi=300, bbox_inches='tight')\n        plt.show()\n        \n        # Detailed statistical analysis\n        print(\"\\n\" + \"=\"*60)\n        print(\"ROC-AUC STABILITY ANALYSIS\")\n        print(\"=\"*60)\n        \n        # Calculate statistics\n        roc_auc_array = np.array(epoch_roc_aucs)\n        \n        print(f\"Number of Epochs Analyzed: {len(roc_auc_array)}\")\n        print(f\"Initial ROC-AUC (Epoch 1): {roc_auc_array[0]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-01T10:36:06.755747Z","iopub.execute_input":"2026-02-01T10:36:06.755977Z","iopub.status.idle":"2026-02-01T10:36:09.260098Z","shell.execute_reply.started":"2026-02-01T10:36:06.755957Z","shell.execute_reply":"2026-02-01T10:36:09.259395Z"}},"outputs":[],"execution_count":null}]}