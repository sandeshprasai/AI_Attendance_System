{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14459907,"sourceType":"datasetVersion","datasetId":9235761}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 0: Suppress ALL TensorFlow/CUDA Warnings (RUN THIS FIRST!)\n# ============================================================================\n\nimport os\nimport sys\nimport warnings\n\n# CRITICAL: Set these BEFORE importing TensorFlow\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Suppress oneDNN messages\nos.environ['TF_CPP_MIN_VLOG_LEVEL'] = '3'  # Suppress verbose logging\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use only first GPU (optional)\n\n# Suppress Python warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Redirect stderr temporarily to suppress XLA messages\nimport io\nimport contextlib\n\n# Suppress all stderr output during TensorFlow import\nstderr_backup = sys.stderr\nsys.stderr = io.StringIO()\n\n# Now import TensorFlow\nimport tensorflow as tf\n\n# Restore stderr\nsys.stderr = stderr_backup\n\n# Additional TensorFlow configuration\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.get_logger().setLevel('ERROR')\ntf.autograph.set_verbosity(0)\n\n# Suppress absl logging\nimport logging\nlogging.getLogger('absl').setLevel(logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\n# Configure GPU memory growth (prevents memory warnings)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… {len(gpus)} GPU(s) configured with memory growth\")\n    except RuntimeError as e:\n        print(f\"GPU configuration: {e}\")\nelse:\n    print(\"âœ… Running on CPU\")\n\nprint(\"âœ… All warnings suppressed successfully\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:21:44.880903Z","iopub.execute_input":"2026-01-28T06:21:44.881342Z","iopub.status.idle":"2026-01-28T06:21:59.306236Z","shell.execute_reply.started":"2026-01-28T06:21:44.881309Z","shell.execute_reply":"2026-01-28T06:21:59.305289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Import Libraries and Suppress Warnings\n# ============================================================================\n\nimport os\nimport warnings\n\n# Suppress TensorFlow warnings FIRST\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nwarnings.filterwarnings('ignore')\n\n# Core imports\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.metrics import (\n    roc_curve, auc, precision_recall_curve, average_precision_score,\n    confusion_matrix, classification_report, f1_score,\n    accuracy_score, precision_score, recall_score,\n    roc_auc_score\n)\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import stats\nimport itertools\nfrom sklearn.calibration import calibration_curve\nfrom tqdm import tqdm\nimport math\nfrom scipy.spatial.distance import pdist, squareform\nfrom itertools import combinations\nfrom sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity\nimport pickle\nimport json\n\n# TensorFlow imports\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nimport logging\n\n# Configure TensorFlow logging\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\nlogging.getLogger('absl').setLevel(logging.ERROR)\n\n# Set plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\nprint(\"âœ… All libraries imported successfully\")\nprint(\"âœ… Warnings suppressed\\n\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:21:59.308062Z","iopub.execute_input":"2026-01-28T06:21:59.308866Z","iopub.status.idle":"2026-01-28T06:22:00.094689Z","shell.execute_reply.started":"2026-01-28T06:21:59.308842Z","shell.execute_reply":"2026-01-28T06:22:00.093959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Global config\nIMG_SIZE = 112\nBATCH_SIZE = 64  # Increased for better gradient estimates\nAUTOTUNE = tf.data.AUTOTUNE\nSEED = 42\n\n# Set random seeds for reproducibility\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Dataset path - UPDATE THIS TO YOUR PATH\nDATASET_DIR = \"/kaggle/input/datasetsforrestnet/ThirdLap\"\n\nprint(\"âœ… Configuration loaded\")\nprint(f\"Image Size: {IMG_SIZE}\")\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Dataset Directory: {DATASET_DIR}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:00.100099Z","iopub.execute_input":"2026-01-28T06:22:00.100375Z","iopub.status.idle":"2026-01-28T06:22:00.105295Z","shell.execute_reply.started":"2026-01-28T06:22:00.100354Z","shell.execute_reply":"2026-01-28T06:22:00.104404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: Data Loading Functions\n# ============================================================================\n\ndef parse_image(file_path, label):\n    \"\"\"Parse and preprocess image\"\"\"\n    image = tf.io.read_file(file_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label\n\n\ndef build_train_val_datasets_class_disjoint(root_dir, val_identities=100):\n    \"\"\"\n    Build datasets with class-disjoint split.\n    Training and validation use COMPLETELY DIFFERENT identities.\n    \n    This is the MOST REALISTIC evaluation:\n    - Training: Person A, B, C (all images)\n    - Validation: Person D, E, F (all images) â† Never seen during training!\n    - Tests TRUE generalization to new identities\n    \n    Args:\n        root_dir: Root directory containing identity folders\n        val_identities: Number of identities to reserve for validation\n    \n    Returns:\n        train_ds, val_ds, num_train_classes\n    \"\"\"\n    class_names = sorted(\n        [d for d in os.listdir(root_dir)\n         if os.path.isdir(os.path.join(root_dir, d))]\n    )\n\n    total_identities = len(class_names)\n    print(f\"Total identities in dataset: {total_identities}\")\n    \n    # Shuffle to randomly assign identities to train/val\n    random.shuffle(class_names)\n    \n    # Split identities (not images!)\n    val_identities = min(val_identities, total_identities // 5)  # At most 20% for val\n    val_classes = class_names[:val_identities]\n    train_classes = class_names[val_identities:]\n    \n    print(f\"\\n{'='*70}\")\n    print(\"CLASS-DISJOINT SPLIT (Most Realistic Evaluation)\")\n    print('='*70)\n    print(f\"Training identities: {len(train_classes)}\")\n    print(f\"Validation identities: {len(val_classes)}\")\n    print(f\"Split: {len(train_classes)}/{len(val_classes)} (train/val)\")\n    print(\"\\nâš ï¸  NOTE: Validation accuracy will be 0% (this is expected!)\")\n    print(\"   The model is trained on different identities than validation.\")\n    print(\"   ROC-AUC is the correct metric to monitor.\\n\")\n\n    # Collect training data (use ALL images from training identities)\n    train_paths, train_labels = [], []\n    for idx, cls in enumerate(train_classes):\n        cls_dir = os.path.join(root_dir, cls)\n        images = [\n            os.path.join(cls_dir, img)\n            for img in os.listdir(cls_dir)\n            if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n        ]\n        \n        train_paths.extend(images)\n        train_labels.extend([idx] * len(images))\n    \n    # Collect validation data (use ALL images from validation identities)\n    val_paths, val_labels = [], []\n    for idx, cls in enumerate(val_classes):\n        cls_dir = os.path.join(root_dir, cls)\n        images = [\n            os.path.join(cls_dir, img)\n            for img in os.listdir(cls_dir)\n            if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n        ]\n        \n        val_paths.extend(images)\n        val_labels.extend([idx] * len(images))\n\n    # ========================================================================\n    # DATA LEAKAGE CHECK - Verify no image appears in both train and val\n    # ========================================================================\n    train_paths_set = set(train_paths)\n    val_paths_set = set(val_paths)\n    overlap = train_paths_set.intersection(val_paths_set)\n\n    print(f\"{'='*70}\")\n    print(\"DATA LEAKAGE VERIFICATION\")\n    print('='*70)\n    print(f\"Training images: {len(train_paths_set):,}\")\n    print(f\"Validation images: {len(val_paths_set):,}\")\n    print(f\"Overlapping images: {len(overlap):,}\")\n\n    if len(overlap) > 0:\n        print(\"\\nðŸš¨ CRITICAL ERROR: DATA LEAKAGE DETECTED!\")\n        print(f\"   {len(overlap)} images appear in BOTH train and validation!\")\n        print(\"   YOUR RESULTS ARE INVALID!\")\n        print(\"\\n   First 5 overlapping files:\")\n        for idx, path in enumerate(list(overlap)[:5]):\n            print(f\"   {idx+1}. {path}\")\n        raise ValueError(\"Data leakage detected! Fix dataset split.\")\n    else:\n        print(\"\\nâœ… PASS: No data leakage detected\")\n        print(\"   Train and validation sets are properly separated\")\n\n    # Verify label overlap (should NOT overlap for class-disjoint)\n    train_labels_set = set(train_labels)\n    val_labels_set = set(val_labels)\n    label_overlap = train_labels_set.intersection(val_labels_set)\n\n    print(f\"\\nLabel overlap: {len(label_overlap)} classes\")\n    if len(label_overlap) == 0:\n        print(\"âœ… CORRECT: Class-disjoint split confirmed\")\n        print(\"   Training and validation use completely different identities\")\n    else:\n        print(\"âš ï¸  WARNING: Labels overlap detected!\")\n        print(f\"   {len(label_overlap)} classes appear in both sets\")\n    print('='*70 + \"\\n\")\n\n    # Create datasets\n    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n    train_ds = train_ds.shuffle(20000, seed=SEED)\n    train_ds = train_ds.map(parse_image, num_parallel_calls=AUTOTUNE)\n    train_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n    val_ds = val_ds.map(parse_image, num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n    num_train_classes = len(train_classes)\n    \n    return train_ds, val_ds, num_train_classes\n\n\nprint(\"âœ… Data loading functions defined\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:00.106335Z","iopub.execute_input":"2026-01-28T06:22:00.106695Z","iopub.status.idle":"2026-01-28T06:22:00.124690Z","shell.execute_reply.started":"2026-01-28T06:22:00.106665Z","shell.execute_reply":"2026-01-28T06:22:00.124095Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #============================================================================\n# CELL 4: Load Dataset\n# ============================================================================\n\nprint(\"Loading datasets with CLASS-DISJOINT split...\")\nprint(\"This measures TRUE generalization to unseen identities!\\n\")\n\ntrain_ds, val_ds, num_classes = build_train_val_datasets_class_disjoint(\n    root_dir=DATASET_DIR,\n    val_identities=100  # Reserve 100 identities for validation\n)\n\nprint(f\"\\nâœ… Datasets loaded successfully\")\nprint(f\"Model will be trained on {num_classes} identities\")\nprint(f\"Validation tests on completely unseen identities\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:00.125653Z","iopub.execute_input":"2026-01-28T06:22:00.125941Z","iopub.status.idle":"2026-01-28T06:22:11.777906Z","shell.execute_reply.started":"2026-01-28T06:22:00.125921Z","shell.execute_reply":"2026-01-28T06:22:11.776999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Model Architecture\n# ============================================================================\n\ndef l2_norm(x):\n    \"\"\"L2 normalization layer\"\"\"\n    return tf.nn.l2_normalize(x, axis=1)\n\n\ndef build_resnet_embedding():\n    \"\"\"Build ResNet50 backbone for face embeddings\"\"\"\n    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\n    \n    # Load ResNet50 with ImageNet weights\n    base = ResNet50(\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=inputs,\n        pooling=\"avg\"\n    )\n    \n    # Fine-tuning strategy: freeze early layers\n    base.trainable = True\n    for layer in base.layers[:100]:\n        layer.trainable = False\n    \n    # Embedding layers\n    x = BatchNormalization(name='bn_before_dense')(base.output)\n    x = Dense(512, use_bias=False, name='dense_embedding')(x)\n    x = BatchNormalization(name='bn_after_dense')(x)\n    embeddings = Lambda(l2_norm, name=\"embeddings\")(x)\n    \n    return Model(inputs, embeddings, name=\"ResNet50_Embedding\")\n\n\nclass ArcFace(tf.keras.layers.Layer):\n    \"\"\"\n    ArcFace layer for face recognition.\n    Adds angular margin to improve discriminative power.\n    \n    Reference: ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n    \"\"\"\n    def __init__(self, num_classes, margin=0.5, scale=64, **kwargs):\n        super(ArcFace, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        self.margin = margin  # Angular margin\n        self.scale = scale    # Feature scale\n        self.cos_m = tf.math.cos(margin)\n        self.sin_m = tf.math.sin(margin)\n        self.threshold = tf.math.cos(math.pi - margin)\n        \n    def build(self, input_shape):\n        self.W = self.add_weight(\n            name=\"W\",\n            shape=(input_shape[-1], self.num_classes),\n            initializer=tf.keras.initializers.glorot_uniform(),\n            regularizer=tf.keras.regularizers.l2(5e-4),\n            trainable=True\n        )\n        super().build(input_shape)\n        \n    def call(self, inputs, labels=None, training=None):\n        # Normalize weights and inputs\n        W_norm = tf.nn.l2_normalize(self.W, axis=0)\n        x_norm = tf.nn.l2_normalize(inputs, axis=1)\n        \n        # Compute cosine similarity\n        cosine = tf.matmul(x_norm, W_norm)\n        \n        if labels is not None and training:\n            # One-hot encode labels\n            one_hot_labels = tf.one_hot(labels, depth=self.num_classes)\n            \n            # Compute theta and add angular margin\n            theta = tf.acos(tf.clip_by_value(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n            target_logit = tf.cos(theta + self.margin)\n            \n            # Combine target and non-target logits\n            logits = cosine * (1 - one_hot_labels) + target_logit * one_hot_labels\n            logits = logits * self.scale\n        else:\n            logits = cosine * self.scale\n        \n        return logits\n    \n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            'num_classes': self.num_classes,\n            'margin': self.margin,\n            'scale': self.scale\n        })\n        return config\n\n\nprint(\"âœ… Model architecture defined\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:11.780412Z","iopub.execute_input":"2026-01-28T06:22:11.780684Z","iopub.status.idle":"2026-01-28T06:22:11.793328Z","shell.execute_reply.started":"2026-01-28T06:22:11.780664Z","shell.execute_reply":"2026-01-28T06:22:11.792529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: Build and Compile Model\n# ============================================================================\n\nprint(\"Building face recognition model...\")\n\n# Build backbone\nbackbone = build_resnet_embedding()\nprint(\"\\nBackbone architecture:\")\nbackbone.summary()\n\n# Build ArcFace layer\narcface = ArcFace(num_classes, margin=0.5, scale=64)\n\n# Create training model\ninputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"input_image\")\nlabels = Input(shape=(), name=\"label\", dtype=tf.int32)\n\nembeddings = backbone(inputs)\nlogits = arcface(embeddings, labels, training=True)\n\n# Training model\ntrain_model = Model([inputs, labels], logits, name=\"ArcFace_Trainer\")\n\n# Compile with fixed learning rate (will be adjusted by ReduceLROnPlateau)\ntrain_model.compile(\n    optimizer=Adam(learning_rate=1e-4),\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)\n\ntrain_model.build([(None, IMG_SIZE, IMG_SIZE, 3), (None,)])\n\nprint(f\"\\nâœ… Training model compiled\")\nprint(f\"Total parameters: {train_model.count_params():,}\")\nprint(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in train_model.trainable_weights]):,}\")\n\n# Create inference model (for extracting embeddings)\ninference_model = Model(inputs=backbone.input, outputs=backbone.output, \n                       name=\"Face_Embedding_Model\")\n\nprint(\"âœ… Inference model created for embedding extraction\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:11.794259Z","iopub.execute_input":"2026-01-28T06:22:11.794676Z","iopub.status.idle":"2026-01-28T06:22:18.214813Z","shell.execute_reply.started":"2026-01-28T06:22:11.794656Z","shell.execute_reply":"2026-01-28T06:22:18.214034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: Prepare Datasets for Training\n# ============================================================================\n\nprint(\"Preparing datasets for training...\")\n\ndef prepare_train_dataset(dataset):\n    \"\"\"Convert dataset to format expected by training model\"\"\"\n    def map_fn(images, labels):\n        return (images, labels), labels\n    return dataset.map(map_fn, num_parallel_calls=AUTOTUNE)\n\n# Prepare datasets\ntrain_ds_for_model = prepare_train_dataset(train_ds)\nval_ds_for_model = prepare_train_dataset(val_ds)\n\nprint(\"âœ… Datasets prepared for ArcFace training\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:18.215732Z","iopub.execute_input":"2026-01-28T06:22:18.215938Z","iopub.status.idle":"2026-01-28T06:22:18.252367Z","shell.execute_reply.started":"2026-01-28T06:22:18.215919Z","shell.execute_reply":"2026-01-28T06:22:18.251621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Training Callbacks\n# ============================================================================\n\nprint(\"Setting up callbacks...\")\n\n# Helper function for ROC-AUC computation\ndef compute_similarity_scores_for_auc(embeddings, labels):\n    \"\"\"Compute similarity scores for ROC-AUC calculation\"\"\"\n    n = len(embeddings)\n    \n    # Sample for efficiency\n    if n > 2000:\n        indices = np.random.choice(n, 2000, replace=False)\n        embeddings = embeddings[indices]\n        labels = labels[indices]\n        n = 2000\n    \n    # Compute cosine similarity\n    similarity_matrix = sklearn_cosine_similarity(embeddings)\n    \n    # Sample pairs\n    scores = []\n    pair_labels = []\n    num_pairs = min(50000, n * (n - 1) // 2)\n    sampled = 0\n    \n    while sampled < num_pairs:\n        i, j = np.random.randint(0, n, 2)\n        if i == j:\n            continue\n        scores.append(similarity_matrix[i, j])\n        pair_labels.append(1 if labels[i] == labels[j] else 0)\n        sampled += 1\n    \n    return np.array(scores), np.array(pair_labels)\n\n\n# Custom callback for ROC-AUC monitoring\nclass ValidationROCAUCCallback(tf.keras.callbacks.Callback):\n    \"\"\"Monitor ROC-AUC on validation set - THE KEY METRIC for class-disjoint validation\"\"\"\n    def __init__(self, backbone, val_ds):\n        super().__init__()\n        self.backbone = backbone\n        self.val_ds = val_ds\n        self.best_auc = 0.0\n        self.auc_history = []\n        \n        # Extract validation labels once\n        print(\"Extracting validation labels...\")\n        self.val_labels = []\n        for _, labels in tqdm(val_ds, desc=\"Getting labels\"):\n            self.val_labels.append(labels.numpy())\n        self.val_labels = np.concatenate(self.val_labels)\n        print(f\"Validation set: {len(self.val_labels)} samples\\n\")\n    \n    def on_epoch_end(self, epoch, logs=None):\n        print(f\"\\n{'='*70}\")\n        print(f\"Epoch {epoch+1}: Computing ROC-AUC on validation set...\")\n        print('='*70)\n        \n        # Extract embeddings\n        embeddings_list = []\n        for images, _ in tqdm(self.val_ds, desc=\"Extracting embeddings\", leave=False):\n            emb = self.backbone(images, training=False)\n            embeddings_list.append(emb.numpy())\n        embeddings = np.vstack(embeddings_list)\n        \n        # Compute ROC-AUC\n        scores, pair_labels = compute_similarity_scores_for_auc(\n            embeddings, self.val_labels\n        )\n        \n        if len(np.unique(pair_labels)) > 1:\n            roc_auc = roc_auc_score(pair_labels, scores)\n            self.auc_history.append(roc_auc)\n            logs['val_roc_auc'] = roc_auc\n            \n            print(f\"ROC-AUC: {roc_auc:.4f}\", end=\"\")\n            \n            if roc_auc > self.best_auc:\n                self.best_auc = roc_auc\n                print(f\" âœ… NEW BEST!\")\n            else:\n                print(f\" (best: {self.best_auc:.4f})\")\n        else:\n            print(\"âš ï¸  Warning: Not enough positive/negative pairs\")\n        \n        print('='*70)\n\n\n# Setup callbacks\ncallbacks = [\n    # ROC-AUC monitoring (MOST IMPORTANT for class-disjoint validation)\n    ValidationROCAUCCallback(backbone, val_ds),\n    \n    # Model checkpoint - save best based on ROC-AUC\n    ModelCheckpoint(\n        'best_arcface_model.keras',\n        monitor='val_roc_auc',\n        mode='max',\n        save_best_only=True,\n        verbose=1\n    ),\n    \n    # Learning rate reduction\n    ReduceLROnPlateau(\n        monitor='val_roc_auc',\n        mode='max',\n        factor=0.5,\n        patience=3,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    \n    # Early stopping\n    EarlyStopping(\n        monitor='val_roc_auc',\n        mode='max',\n        patience=8,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    \n    # TensorBoard\n    tf.keras.callbacks.TensorBoard(\n        log_dir='./logs',\n        histogram_freq=1\n    )\n]\n\n\nprint(\"âœ… Callbacks configured\")\nprint(\"ðŸ”‘ KEY: Monitoring 'val_roc_auc' (ignore val_accuracy, it will be 0%)\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:22:18.254096Z","iopub.execute_input":"2026-01-28T06:22:18.254471Z","iopub.status.idle":"2026-01-28T06:23:21.699123Z","shell.execute_reply.started":"2026-01-28T06:22:18.254448Z","shell.execute_reply":"2026-01-28T06:23:21.698429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: Train the Model\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*70)\nprint(\"\\nâš ï¸  REMINDER: val_accuracy will be 0% (this is NORMAL)\")\nprint(\"   We're testing on completely unseen identities.\")\nprint(\"   ROC-AUC is the metric that matters!\\n\")\n\nEPOCHS = 40\n\nhistory = train_model.fit(\n    train_ds_for_model,\n    validation_data=val_ds_for_model,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=1  # Cleaner output: one line per epoch\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… TRAINING COMPLETED\")\nprint(\"=\"*70)\nif callbacks[0].auc_history:\n    print(f\"Best ROC-AUC: {max(callbacks[0].auc_history):.4f}\")\n    print(f\"Final ROC-AUC: {callbacks[0].auc_history[-1]:.4f}\")\nprint(\"=\"*70 + \"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T06:23:21.699966Z","iopub.execute_input":"2026-01-28T06:23:21.700715Z","iopub.status.idle":"2026-01-28T10:18:13.789265Z","shell.execute_reply.started":"2026-01-28T06:23:21.700692Z","shell.execute_reply":"2026-01-28T10:18:13.788557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Save Model and History\n# ============================================================================\n\nprint(\"Saving model and training history...\")\n\n# Save training history\nhistory_dict = {}\nfor key, values in history.history.items():\n    if isinstance(values, list):\n        history_dict[key] = [float(v) for v in values]\n\nwith open('training_history.json', 'w') as f:\n    json.dump(history_dict, f, indent=4)\n\nwith open('training_history.pkl', 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Save complete model\ntrain_model.save(\"arcface_resnet50_final.keras\")\n\n# Save only the backbone for inference\ninference_model.save(\"embedding_model.keras\")\n\nprint(\"âœ… Models and history saved\")\nprint(\"Files created:\")\nprint(\"  - best_arcface_model.keras (best checkpoint)\")\nprint(\"  - arcface_resnet50_final.keras (final model)\")\nprint(\"  - embedding_model.keras (inference only)\")\nprint(\"  - training_history.json\")\nprint(\"  - training_history.pkl\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:18:13.790425Z","iopub.execute_input":"2026-01-28T10:18:13.790992Z","iopub.status.idle":"2026-01-28T10:18:15.802815Z","shell.execute_reply.started":"2026-01-28T10:18:13.790968Z","shell.execute_reply":"2026-01-28T10:18:15.801965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Evaluation Functions\n# ============================================================================\n\ndef extract_embeddings(model, dataset):\n    \"\"\"Extract embeddings from dataset\"\"\"\n    embeddings_list = []\n    labels_list = []\n    \n    print(\"Extracting embeddings...\")\n    for images, labels in tqdm(dataset, desc=\"Processing batches\"):\n        embeddings = model.predict(images, verbose=0)\n        embeddings_list.append(embeddings)\n        labels_list.append(labels.numpy())\n    \n    return np.vstack(embeddings_list), np.concatenate(labels_list)\n\n\ndef compute_similarity_scores(embeddings, labels):\n    \"\"\"Compute similarity scores for verification\"\"\"\n    n = len(embeddings)\n    \n    # Sample for efficiency if too many\n    if n > 2000:\n        indices = np.random.choice(n, 2000, replace=False)\n        embeddings = embeddings[indices]\n        labels = labels[indices]\n        n = 2000\n    \n    similarity_matrix = sklearn_cosine_similarity(embeddings)\n    \n    # Create pair labels\n    pair_labels = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            pair_labels[i, j] = 1 if labels[i] == labels[j] else 0\n    \n    # Get upper triangle (excluding diagonal)\n    mask = np.triu_indices(n, k=1)\n    scores = similarity_matrix[mask]\n    labels_flat = pair_labels[mask]\n    \n    return scores, labels_flat\n\n\nprint(\"âœ… Evaluation functions defined\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:18:15.803654Z","iopub.execute_input":"2026-01-28T10:18:15.804008Z","iopub.status.idle":"2026-01-28T10:18:15.811657Z","shell.execute_reply.started":"2026-01-28T10:18:15.803982Z","shell.execute_reply":"2026-01-28T10:18:15.810952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 12: Extract Embeddings and Compute Metrics\n# ============================================================================\n\nprint(\"=\"*70)\nprint(\"EVALUATION: Extracting embeddings and computing metrics\")\nprint(\"=\"*70)\n\n# Extract validation embeddings\nprint(\"\\nExtracting validation embeddings...\")\nval_embeddings, val_labels = extract_embeddings(inference_model, val_ds)\n\nprint(f\"\\nValidation set:\")\nprint(f\"  Embeddings: {val_embeddings.shape}\")\nprint(f\"  Unique identities: {len(np.unique(val_labels))}\")\n\n# Compute similarity scores\nprint(\"\\nComputing similarity scores...\")\nscores, pair_labels = compute_similarity_scores(val_embeddings, val_labels)\n\n# Compute ROC curve\nprint(\"Computing ROC curve...\")\nfpr, tpr, thresholds = roc_curve(pair_labels, scores)\nroc_auc = auc(fpr, tpr)\n\n# Compute Precision-Recall curve\nprint(\"Computing Precision-Recall curve...\")\nprecision, recall, pr_thresholds = precision_recall_curve(pair_labels, scores)\navg_precision = average_precision_score(pair_labels, scores)\n\n# Find optimal threshold\nprint(\"Finding optimal threshold...\")\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\nbest_f1_idx = np.argmax(f1_scores)\nbest_threshold = pr_thresholds[best_f1_idx] if best_f1_idx < len(pr_thresholds) else 0.5\nbest_f1 = f1_scores[best_f1_idx]\n\n# Compute TAR at specific FARs\ntar_at_far = {}\nfor far_target in [0.001, 0.0001]:\n    idx = np.argmin(np.abs(fpr - far_target))\n    tar_at_far[far_target] = tpr[idx]\n    threshold_at_far = thresholds[idx]\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*70)\nprint(f\"ROC-AUC: {roc_auc:.4f}\")\nprint(f\"Average Precision: {avg_precision:.4f}\")\nprint(f\"Best F1 Score: {best_f1:.4f} (threshold: {best_threshold:.4f})\")\nprint(f\"\\nTrue Acceptance Rate (TAR) at Fixed False Acceptance Rate (FAR):\")\nprint(f\"  TAR @ FAR=0.1%:  {tar_at_far[0.001]:.4f}\")\nprint(f\"  TAR @ FAR=0.01%: {tar_at_far[0.0001]:.4f}\")\nprint(\"=\"*70 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:18:15.812636Z","iopub.execute_input":"2026-01-28T10:18:15.812906Z","iopub.status.idle":"2026-01-28T10:19:23.018184Z","shell.execute_reply.started":"2026-01-28T10:18:15.812875Z","shell.execute_reply":"2026-01-28T10:19:23.017403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 13: Plot Training History\n# ============================================================================\n\nprint(\"Plotting training history...\")\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nepochs_range = range(1, len(history.history['accuracy']) + 1)\n\n# Plot 1: Training & Validation Accuracy\naxes[0, 0].plot(epochs_range, history.history['accuracy'], 'b-', linewidth=2, label='Training')\naxes[0, 0].plot(epochs_range, history.history['val_accuracy'], 'r-', linewidth=2, label='Validation')\naxes[0, 0].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\naxes[0, 0].set_xlabel('Epochs')\naxes[0, 0].set_ylabel('Accuracy')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].text(0.5, 0.5, 'Note: Val accuracy â‰ˆ 0%\\n(Class-disjoint validation)', \n                transform=axes[0, 0].transAxes, fontsize=10, \n                ha='center', va='center', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n\n# Plot 2: Training & Validation Loss\naxes[0, 1].plot(epochs_range, history.history['loss'], 'b-', linewidth=2, label='Training')\naxes[0, 1].plot(epochs_range, history.history['val_loss'], 'r-', linewidth=2, label='Validation')\naxes[0, 1].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\naxes[0, 1].set_xlabel('Epochs')\naxes[0, 1].set_ylabel('Loss')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: ROC-AUC Over Time\nif 'val_roc_auc' in history.history:\n    axes[1, 0].plot(epochs_range, history.history['val_roc_auc'], 'g-', linewidth=2)\n    axes[1, 0].set_title('Validation ROC-AUC Over Time', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epochs')\n    axes[1, 0].set_ylabel('ROC-AUC')\n    axes[1, 0].grid(True, alpha=0.3)\n    \n    # Mark best\n    best_epoch = np.argmax(history.history['val_roc_auc'])\n    best_auc = history.history['val_roc_auc'][best_epoch]\n    axes[1, 0].scatter(best_epoch + 1, best_auc, color='red', s=200, zorder=5)\n    axes[1, 0].annotate(f'Best: {best_auc:.4f}', \n                       xy=(best_epoch + 1, best_auc),\n                       xytext=(10, 10), textcoords='offset points',\n                       bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))\n\n# Plot 4: Learning Rate Schedule\nif 'lr' in history.history:\n    axes[1, 1].plot(epochs_range, history.history['lr'], 'purple', linewidth=2)\n    axes[1, 1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Epochs')\n    axes[1, 1].set_ylabel('Learning Rate')\n    axes[1, 1].set_yscale('log')\n    axes[1, 1].grid(True, alpha=0.3)\nelse:\n    axes[1, 1].axis('off')\n\nplt.tight_layout()\nplt.savefig('training_history.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… Training history plot saved as 'training_history.png'\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:19:23.019238Z","iopub.execute_input":"2026-01-28T10:19:23.019606Z","iopub.status.idle":"2026-01-28T10:19:25.114002Z","shell.execute_reply.started":"2026-01-28T10:19:23.019583Z","shell.execute_reply":"2026-01-28T10:19:25.113077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 14: Plot ROC Curve\n# ============================================================================\n\nprint(\"Plotting ROC curve...\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, \n         label=f'ROC curve (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n         label='Random Classifier')\n\n# Mark important points\nfor far_target in [0.001, 0.0001]:\n    idx = np.argmin(np.abs(fpr - far_target))\n    plt.scatter(fpr[idx], tpr[idx], s=100, zorder=5)\n    plt.annotate(f'FAR={far_target:.4f}\\nTAR={tpr[idx]:.4f}', \n                xy=(fpr[idx], tpr[idx]),\n                xytext=(10, -10), textcoords='offset points',\n                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)', fontsize=12)\nplt.ylabel('True Positive Rate (TPR)', fontsize=12)\nplt.title('ROC Curve - Face Verification Performance', fontsize=14, fontweight='bold')\nplt.legend(loc=\"lower right\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… ROC curve saved as 'roc_curve.png'\\n\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:19:25.115145Z","iopub.execute_input":"2026-01-28T10:19:25.115467Z","iopub.status.idle":"2026-01-28T10:19:25.960283Z","shell.execute_reply.started":"2026-01-28T10:19:25.115443Z","shell.execute_reply":"2026-01-28T10:19:25.959286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 15: Plot Precision-Recall Curve\n# ============================================================================\n\nprint(\"Plotting Precision-Recall curve...\")\n\nplt.figure(figsize=(10, 8))\nplt.plot(recall, precision, color='darkgreen', lw=2, \n         label=f'AP = {avg_precision:.4f}')\nplt.fill_between(recall, precision, alpha=0.2, color='green')\n\n# Add F1-score contours\nf1_scores_plot = np.linspace(0.1, 0.9, 9)\nfor f1 in f1_scores_plot:\n    x = np.linspace(0.01, 1)\n    y = f1 * x / (2 * x - f1)\n    y = np.where(y >= 0, y, np.nan)\n    plt.plot(x, y, color='gray', alpha=0.2, linestyle='--', linewidth=1)\n    if f1 in [0.3, 0.5, 0.7, 0.9]:\n        plt.text(0.9, y[-1] - 0.03, f'F1={f1:.1f}', fontsize=8, alpha=0.5)\n\nplt.xlabel('Recall', fontsize=12)\nplt.ylabel('Precision', fontsize=12)\nplt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.legend(loc=\"lower left\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… Precision-Recall curve saved as 'precision_recall_curve.png'\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:54:08.687104Z","iopub.execute_input":"2026-01-28T10:54:08.687768Z","iopub.status.idle":"2026-01-28T10:54:14.196397Z","shell.execute_reply.started":"2026-01-28T10:54:08.687739Z","shell.execute_reply":"2026-01-28T10:54:14.195343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 16: Plot Similarity Distribution\n# ============================================================================\n\nprint(\"Plotting similarity distribution...\")\n\n# Separate positive and negative pairs\npos_scores = scores[pair_labels == 1]\nneg_scores = scores[pair_labels == 0]\n\nplt.figure(figsize=(16, 6))\n\n# Histogram\nplt.subplot(1, 2, 1)\nplt.hist(pos_scores, bins=100, alpha=0.6, color='blue', \n         label=f'Intra-class (n={len(pos_scores):,})', density=True)\nplt.hist(neg_scores, bins=100, alpha=0.6, color='red', \n         label=f'Inter-class (n={len(neg_scores):,})', density=True)\nplt.xlabel('Cosine Similarity', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.title('Intra vs Inter-Class Similarity Distribution', fontsize=14, fontweight='bold')\nplt.legend(loc='upper left')\nplt.grid(True, alpha=0.3)\n\n# Statistics - moved to a better location\npos_mean = np.mean(pos_scores)\npos_std = np.std(pos_scores)\nneg_mean = np.mean(neg_scores)\nneg_std = np.std(neg_scores)\n\n# Find empty space in the plot for text placement\n# Place text in upper right corner with better formatting\nstats_text = (f'Intra-class (Same):\\n'\n              f'  Mean: {pos_mean:.4f}\\n'\n              f'  Std: {pos_std:.4f}\\n'\n              f'  Min: {np.min(pos_scores):.4f}\\n'\n              f'  Max: {np.max(pos_scores):.4f}\\n\\n'\n              f'Inter-class (Different):\\n'\n              f'  Mean: {neg_mean:.4f}\\n'\n              f'  Std: {neg_std:.4f}\\n'\n              f'  Min: {np.min(neg_scores):.4f}\\n'\n              f'  Max: {np.max(neg_scores):.4f}')\n\nplt.text(0.98, 0.98, stats_text,\n         transform=plt.gca().transAxes, fontsize=9,\n         verticalalignment='top', horizontalalignment='right',\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, pad=0.5),\n         family='monospace')  # Monospace font for alignment\n\n# Box plot\nplt.subplot(1, 2, 2)\nbox_data = [pos_scores, neg_scores]\nbp = plt.boxplot(box_data, labels=['Intra-class\\n(Same)', 'Inter-class\\n(Different)'], \n                 patch_artist=True, widths=0.6)\nbp['boxes'][0].set_facecolor('lightblue')\nbp['boxes'][1].set_facecolor('lightcoral')\n\n# Add mean markers\nplt.scatter([1], [pos_mean], color='darkblue', s=100, zorder=3, marker='_', linewidth=2, label='Mean')\nplt.scatter([2], [neg_mean], color='darkred', s=100, zorder=3, marker='_', linewidth=2)\n\nplt.ylabel('Cosine Similarity', fontsize=12)\nplt.title('Similarity Distribution Comparison', fontsize=14, fontweight='bold')\nplt.grid(True, alpha=0.3, axis='y')\n\n# Add statistics to box plot as well\nplt.text(0.02, 0.98, f'Î” Mean: {pos_mean - neg_mean:.4f}',\n         transform=plt.gca().transAxes, fontsize=10,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n\nplt.tight_layout()\nplt.savefig('similarity_distribution.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"âœ… Similarity distribution saved as 'similarity_distribution.png'\\n\")\n\n# Additional statistics printout\nprint(\"=\"*60)\nprint(\"SIMILARITY STATISTICS SUMMARY:\")\nprint(\"=\"*60)\nprint(f\"Intra-class (Same Class Pairs):\")\nprint(f\"  Count: {len(pos_scores):,}\")\nprint(f\"  Mean Â± Std: {pos_mean:.4f} Â± {pos_std:.4f}\")\nprint(f\"  Range: [{np.min(pos_scores):.4f}, {np.max(pos_scores):.4f}]\")\nprint(f\"  Median: {np.median(pos_scores):.4f}\")\nprint()\nprint(f\"Inter-class (Different Class Pairs):\")\nprint(f\"  Count: {len(neg_scores):,}\")\nprint(f\"  Mean Â± Std: {neg_mean:.4f} Â± {neg_std:.4f}\")\nprint(f\"  Range: [{np.min(neg_scores):.4f}, {np.max(neg_scores):.4f}]\")\nprint(f\"  Median: {np.median(neg_scores):.4f}\")\nprint()\nprint(f\"Separation: {pos_mean - neg_mean:.4f} (higher is better)\")\nprint(f\"Overlap area (approx): {np.sum(pos_scores < neg_mean) / len(pos_scores) * 100:.2f}%\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:47:22.835183Z","iopub.execute_input":"2026-01-28T10:47:22.836105Z","iopub.status.idle":"2026-01-28T10:47:24.327276Z","shell.execute_reply.started":"2026-01-28T10:47:22.836076Z","shell.execute_reply":"2026-01-28T10:47:24.326552Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_tar_far_det(scores, pair_labels):\n    \"\"\"\n    scores       : cosine similarity scores\n    pair_labels  : 1 = same identity, 0 = different identity\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(pair_labels, scores)\n\n    plt.figure(figsize=(8, 6))\n    plt.semilogx(fpr, tpr, linewidth=2)\n    plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.6)\n\n    plt.xlabel(\"False Acceptance Rate (FAR) [log scale]\")\n    plt.ylabel(\"True Acceptance Rate (TAR)\")\n    plt.title(\"TAR vs FAR Curve (DET-style)\")\n\n    # Mark standard biometric operating points\n    for far in [1e-2, 1e-3, 1e-4]:\n        idx = np.argmin(np.abs(fpr - far))\n        plt.scatter(fpr[idx], tpr[idx])\n        plt.text(\n            fpr[idx], tpr[idx],\n            f\"FAR={far:.0e}\\nTAR={tpr[idx]:.3f}\",\n            fontsize=9\n        )\n\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:09:38.697257Z","iopub.execute_input":"2026-01-28T11:09:38.698010Z","iopub.status.idle":"2026-01-28T11:09:38.703616Z","shell.execute_reply.started":"2026-01-28T11:09:38.697982Z","shell.execute_reply":"2026-01-28T11:09:38.702852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_threshold_vs_far_tar(scores, pair_labels):\n    fpr, tpr, thresholds = roc_curve(pair_labels, scores)\n\n    plt.figure(figsize=(9, 6))\n    plt.plot(thresholds, fpr, label=\"FAR\", linewidth=2)\n    plt.plot(thresholds, tpr, label=\"TAR\", linewidth=2)\n\n    plt.xlabel(\"Similarity Threshold\")\n    plt.ylabel(\"Rate\")\n    plt.title(\"Threshold vs FAR / TAR\")\n    plt.grid(True, linestyle=\"--\", alpha=0.6)\n    plt.legend()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:09:40.605055Z","iopub.execute_input":"2026-01-28T11:09:40.605520Z","iopub.status.idle":"2026-01-28T11:09:40.610861Z","shell.execute_reply.started":"2026-01-28T11:09:40.605473Z","shell.execute_reply":"2026-01-28T11:09:40.610009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_tar_far_det(scores, pair_labels)\nplot_threshold_vs_far_tar(scores, pair_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:09:50.723982Z","iopub.execute_input":"2026-01-28T11:09:50.724468Z","iopub.status.idle":"2026-01-28T11:09:52.217227Z","shell.execute_reply.started":"2026-01-28T11:09:50.724441Z","shell.execute_reply":"2026-01-28T11:09:52.216460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\ndef compute_intra_class_variance(embeddings, labels):\n    \"\"\"\n    embeddings : (N, 512)\n    labels     : identity labels\n    \"\"\"\n    identity_embs = defaultdict(list)\n\n    for emb, lbl in zip(embeddings, labels):\n        identity_embs[lbl].append(emb)\n\n    identity_variance = {}\n    for lbl, embs in identity_embs.items():\n        embs = np.stack(embs)\n        centroid = np.mean(embs, axis=0)\n        distances = np.linalg.norm(embs - centroid, axis=1)\n        identity_variance[lbl] = distances\n\n    return identity_variance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:06:53.962388Z","iopub.execute_input":"2026-01-28T11:06:53.963044Z","iopub.status.idle":"2026-01-28T11:06:53.968552Z","shell.execute_reply.started":"2026-01-28T11:06:53.963015Z","shell.execute_reply":"2026-01-28T11:06:53.967714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_intra_class_variance_boxplot(identity_variance, top_k=30):\n    \"\"\"\n    Visualizes Top-K identities with highest intra-class variance\n    in a thesis-quality, readable format.\n    \"\"\"\n\n    # Sort identities by mean variance (descending)\n    sorted_items = sorted(\n        identity_variance.items(),\n        key=lambda x: np.mean(x[1]),\n        reverse=True\n    )[:top_k]\n\n    labels = [str(k) for k, _ in sorted_items]\n    data = [v for _, v in sorted_items]\n    means = [np.mean(v) for v in data]\n\n    fig, ax = plt.subplots(figsize=(13, 0.45 * top_k + 2))\n\n    box = ax.boxplot(\n        data,\n        vert=False,\n        showfliers=False,\n        patch_artist=True,\n        widths=0.6\n    )\n\n    # Subtle coloring (professional look)\n    for patch in box[\"boxes\"]:\n        patch.set_facecolor(\"#dbeafe\")   # light blue\n        patch.set_edgecolor(\"#1e40af\")   # dark blue\n        patch.set_linewidth(1.2)\n\n    # Median line styling\n    for median in box[\"medians\"]:\n        median.set_color(\"#dc2626\")      # red\n        median.set_linewidth(1.5)\n\n    # Mean variance overlay\n    ax.scatter(\n        means,\n        range(1, top_k + 1),\n        color=\"#065f46\",\n        marker=\"D\",\n        s=35,\n        label=\"Mean Variance\"\n    )\n\n    ax.set_yticks(range(1, top_k + 1))\n    ax.set_yticklabels(labels, fontsize=9)\n    ax.set_xlabel(\"Embedding Distance from Class Centroid\", fontsize=11)\n    ax.set_title(\n        f\"Top-{top_k} Identities with Highest Intra-Class Variance\",\n        fontsize=13,\n        pad=12\n    )\n\n    ax.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n    ax.legend(loc=\"lower right\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:10:59.530482Z","iopub.execute_input":"2026-01-28T11:10:59.531065Z","iopub.status.idle":"2026-01-28T11:10:59.539127Z","shell.execute_reply.started":"2026-01-28T11:10:59.531037Z","shell.execute_reply":"2026-01-28T11:10:59.538433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"identity_variance = compute_intra_class_variance(\n    embeddings=val_embeddings,\n    labels=val_labels\n)\n\nplot_intra_class_variance_boxplot(identity_variance, top_k=30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:11:01.894674Z","iopub.execute_input":"2026-01-28T11:11:01.895414Z","iopub.status.idle":"2026-01-28T11:11:02.372668Z","shell.execute_reply.started":"2026-01-28T11:11:01.895368Z","shell.execute_reply":"2026-01-28T11:11:02.371706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 17: Summary Report\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL PERFORMANCE SUMMARY\")\nprint(\"=\"*70)\nprint(\"\\nðŸ“Š Key Metrics:\")\nprint(f\"  ROC-AUC:              {roc_auc:.4f}\")\nprint(f\"  Average Precision:    {avg_precision:.4f}\")\nprint(f\"  Best F1 Score:        {best_f1:.4f}\")\nprint(f\"  Optimal Threshold:    {best_threshold:.4f}\")\n\nprint(\"\\nðŸŽ¯ Performance at Different Security Levels:\")\nprint(f\"  TAR @ FAR=0.1%:       {tar_at_far[0.001]:.4f}  (1 in 1,000)\")\nprint(f\"  TAR @ FAR=0.01%:      {tar_at_far[0.0001]:.4f}  (1 in 10,000)\")\n\nprint(\"\\nðŸ“ˆ Training Summary:\")\nprint(f\"  Total epochs:         {len(history.history['accuracy'])}\")\nif 'val_roc_auc' in history.history:\n    print(f\"  Best ROC-AUC:         {max(history.history['val_roc_auc']):.4f}\")\n    print(f\"  Final ROC-AUC:        {history.history['val_roc_auc'][-1]:.4f}\")\n\nprint(\"\\nðŸ“ Generated Files:\")\nprint(\"  - best_arcface_model.keras\")\nprint(\"  - embedding_model.keras\")\nprint(\"  - training_history.json\")\nprint(\"  - training_history.png\")\nprint(\"  - roc_curve.png\")\nprint(\"  - precision_recall_curve.png\")\nprint(\"  - similarity_distribution.png\")\n\nprint(\"\\nðŸ’¡ Interpretation:\")\nif roc_auc >= 0.90:\n    print(\"  â­â­â­â­â­ EXCELLENT - Model shows strong generalization!\")\nelif roc_auc >= 0.85:\n    print(\"  â­â­â­â­ VERY GOOD - Model is production-ready!\")\nelif roc_auc >= 0.80:\n    print(\"  â­â­â­ GOOD - Model works well, room for improvement\")\nelse:\n    print(\"  â­â­ FAIR - Model needs improvement\")\n\nprint(\"\\nðŸŽ¯ Recommended Use Cases:\")\nif tar_at_far[0.001] >= 0.80:\n    print(\"  âœ… Mobile phone unlock\")\n    print(\"  âœ… Access control systems\")\n    print(\"  âœ… Photo organization\")\nif tar_at_far[0.0001] >= 0.70:\n    print(\"  âœ… Payment authentication (with fallback)\")\n    print(\"  âœ… Secure facility access\")\n\nprint(\"\\nâš ï¸  Important Notes:\")\nprint(\"  - This is class-disjoint validation (most realistic)\")\nprint(\"  - Validation accuracy â‰ˆ 0% is EXPECTED (different identities)\")\nprint(\"  - ROC-AUC measures ability to distinguish same vs. different person\")\nprint(\"  - Real-world performance depends on image quality and conditions\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… EVALUATION COMPLETE!\")\nprint(\"=\"*70 + \"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:19:33.004262Z","iopub.execute_input":"2026-01-28T10:19:33.004604Z","iopub.status.idle":"2026-01-28T10:19:33.013847Z","shell.execute_reply.started":"2026-01-28T10:19:33.004570Z","shell.execute_reply":"2026-01-28T10:19:33.013093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# OPTIONAL CELL 18: Save Embeddings for Future Use\n# ============================================================================\n\nprint(\"Saving validation embeddings for future analysis...\")\n\n# Save embeddings and labels\nnp.save('val_embeddings.npy', val_embeddings)\nnp.save('val_labels.npy', val_labels)\n\n# Save metrics\nmetrics = {\n    'roc_auc': float(roc_auc),\n    'avg_precision': float(avg_precision),\n    'best_f1': float(best_f1),\n    'best_threshold': float(best_threshold),\n    'tar_at_far': {str(k): float(v) for k, v in tar_at_far.items()},\n    'intra_class_mean': float(np.mean(pos_scores)),\n    'inter_class_mean': float(np.mean(neg_scores))\n}\n\nwith open('evaluation_metrics.json', 'w') as f:\n    json.dump(metrics, f, indent=4)\n\nprint(\"âœ… Embeddings and metrics saved\")\nprint(\"  - val_embeddings.npy\")\nprint(\"  - val_labels.npy\")\nprint(\"  - evaluation_metrics.json\\n\")\n\nprint(\"=\"*70)\nprint(\"ðŸŽ‰ PIPELINE COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nAll training, evaluation, and visualization steps completed successfully!\")\nprint(\"Your model is ready for deployment or further analysis.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T10:19:33.014828Z","iopub.execute_input":"2026-01-28T10:19:33.015139Z","iopub.status.idle":"2026-01-28T10:19:33.065222Z","shell.execute_reply.started":"2026-01-28T10:19:33.015112Z","shell.execute_reply":"2026-01-28T10:19:33.064553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r final_year_project_face_recognition.zip \\\nbest_arcface_model.keras \\\nembedding_model.keras \\\nevaluation_metrics.json \\\nroc_curve.png \\\nprecision_recall_curve.png \\\nsimilarity_distribution.png \\\ntraining_history.png \\\nval_embeddings.npy \\\nval_labels.npy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:14:54.474986Z","iopub.execute_input":"2026-01-28T11:14:54.475873Z","iopub.status.idle":"2026-01-28T11:15:15.946283Z","shell.execute_reply.started":"2026-01-28T11:14:54.475844Z","shell.execute_reply":"2026-01-28T11:15:15.945439Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === One-cell setup for ResNet Class-Disjoint Model artifacts ===\n\n# 1. Create folder\n!mkdir -p \"ResNet_Class_Disjoint_Model\"\n\n# 2. Copy required files into the folder\n!cp best_arcface_model.keras ResNet_Class_Disjoint_Model/\n!cp embedding_model.keras ResNet_Class_Disjoint_Model/\n!cp evaluation_metrics.json ResNet_Class_Disjoint_Model/\n!cp roc_curve.png ResNet_Class_Disjoint_Model/\n!cp precision_recall_curve.png ResNet_Class_Disjoint_Model/\n!cp similarity_distribution.png ResNet_Class_Disjoint_Model/\n!cp training_history.png ResNet_Class_Disjoint_Model/\n!cp val_embeddings.npy ResNet_Class_Disjoint_Model/\n!cp val_labels.npy ResNet_Class_Disjoint_Model/\n\n# 3. Verify contents\nprint(\"ðŸ“ Folder contents:\")\n!ls -lh ResNet_Class_Disjoint_Model\n\n# 4. (Optional but recommended) Zip the folder for easy download\n!zip -r ResNet_Class_Disjoint_Model.zip ResNet_Class_Disjoint_Model\n\nprint(\"\\nâœ… Setup complete.\")\nprint(\"âž¡ï¸ Now click: Save Version â†’ Save Output to persist for next session.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:24:40.739409Z","iopub.execute_input":"2026-01-28T11:24:40.740201Z","iopub.status.idle":"2026-01-28T11:25:04.759414Z","shell.execute_reply.started":"2026-01-28T11:24:40.740166Z","shell.execute_reply":"2026-01-28T11:25:04.758561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}