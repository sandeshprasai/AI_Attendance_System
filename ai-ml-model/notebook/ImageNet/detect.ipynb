{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dc2913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "918d856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class L2Normalize(layers.Layer):\n",
    "    def __init__(self, axis=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=self.axis)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"axis\": self.axis})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "094f2b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ImageNet embedding model loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"face_embedding\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"face_embedding\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ face_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ir_resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc_embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,048,576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn_embed (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ face_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ ir_resnet50 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fc_embed (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,048,576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bn_embed (\u001b[38;5;33mBatchNormalization\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ l2_norm (\u001b[38;5;33mL2Normalize\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,638,336</span> (93.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,638,336\u001b[0m (93.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,638,336</span> (93.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m24,638,336\u001b[0m (93.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_PATH = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/src/models/ImageNetModel/face_embedding.keras\"\n",
    "\n",
    "embedding_model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\"L2Normalize\": L2Normalize},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "print(\"✅ ImageNet embedding model loaded successfully\")\n",
    "embedding_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e6cc096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_face(img_bgr):\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (112, 112), interpolation=cv2.INTER_AREA)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)  # (1,112,112,3)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf18db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "PERSON_FOLDER = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/DataSets/SandeshPrasai\"\n",
    "PERSON_NAME = \"Sandesh Prasai\"\n",
    "\n",
    "def extract_person_embedding(folder_path, model):\n",
    "    embeddings = []\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "    for img_name in tqdm(image_files):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        face_input = preprocess_face(img)\n",
    "        emb = model.predict(face_input, verbose=0)[0]\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = normalize(np.array(embeddings), axis=1)\n",
    "    return embeddings.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8793db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reference embedding saved: (512,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reference_embedding = extract_person_embedding(PERSON_FOLDER, embedding_model)\n",
    "np.save(f\"{PERSON_NAME}_imagenet_embedding.npy\", reference_embedding)\n",
    "\n",
    "print(\"✅ Reference embedding saved:\", reference_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0279584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_FACIAL_POINTS = np.array([\n",
    "    [38.2946, 51.6963],\n",
    "    [73.5318, 51.5014],\n",
    "    [56.0252, 71.7366],\n",
    "    [41.5493, 92.3655],\n",
    "    [70.7299, 92.2041]\n",
    "], dtype=np.float32)\n",
    "\n",
    "def align_face_to_template(img, landmarks):\n",
    "    src_pts = np.array([\n",
    "        landmarks['left_eye'],\n",
    "        landmarks['right_eye'],\n",
    "        landmarks['nose'],\n",
    "        landmarks['mouth_left'],\n",
    "        landmarks['mouth_right']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    if src_pts.shape != (5, 2):\n",
    "        return None\n",
    "\n",
    "    tform, _ = cv2.estimateAffinePartial2D(src_pts, REFERENCE_FACIAL_POINTS)\n",
    "    if tform is None:\n",
    "        return None\n",
    "\n",
    "    return cv2.warpAffine(img, tform, (112, 112), borderValue=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "409d04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "PERSON_FOLDER = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/DataSets/SandeshPrasai\"\n",
    "PERSON_NAME = \"Sandesh Prasai\"\n",
    "\n",
    "def extract_person_embedding(folder_path, model):\n",
    "    embeddings = []\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "    for img_name in tqdm(image_files):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        face_input = preprocess_face(img)\n",
    "        emb = model.predict(face_input, verbose=0)[0]\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    embeddings = normalize(np.array(embeddings), axis=1)\n",
    "    return embeddings.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "65fc44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECT_SIZE = 320  # 240–360 is ideal\n",
    "\n",
    "def resize_for_detection(frame, target=320):\n",
    "    h, w = frame.shape[:2]\n",
    "    scale = target / max(h, w)\n",
    "    resized = cv2.resize(frame, (int(w*scale), int(h*scale)))\n",
    "    return resized, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b0df706",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECT_EVERY = 5   # detect once every 5 frames\n",
    "frame_count = 0\n",
    "cached_face = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a5bcef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 12:16:11.176942: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,24,24,3] vs. [1,1,1,28]\n",
      "2026-02-04 12:16:43.701530: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,24,24,3] vs. [1,1,1,28]\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "\n",
    "detector = MTCNN(device=\"cpu\")\n",
    "\n",
    "reference_embedding = np.load(\"Sandesh Prasai_imagenet_embedding.npy\")\n",
    "reference_embedding = reference_embedding / np.linalg.norm(reference_embedding)\n",
    "\n",
    "THRESHOLD = 0.9\n",
    "DETECT_EVERY = 5\n",
    "DETECT_SIZE = 320\n",
    "\n",
    "cap = cv2.VideoCapture(\"http://192.168.1.75:4747/video\")\n",
    "\n",
    "frame_count = 0\n",
    "cached_face = None\n",
    "\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # --- Face detection (sparse) ---\n",
    "    if frame_count % DETECT_EVERY == 0 or cached_face is None:\n",
    "        small, scale = resize_for_detection(frame, DETECT_SIZE)\n",
    "        rgb_small = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)\n",
    "        results = detector.detect_faces(rgb_small)\n",
    "\n",
    "        if results:\n",
    "            cached_face = max(results, key=lambda x: x['confidence'])\n",
    "            cached_scale = scale\n",
    "        else:\n",
    "            cached_face = None\n",
    "\n",
    "    # --- Face verification ---\n",
    "    if cached_face:\n",
    "        box = cached_face['box']\n",
    "        keypoints = cached_face['keypoints']\n",
    "\n",
    "        # Rescale landmarks\n",
    "        kp = {k: (int(v[0]/cached_scale), int(v[1]/cached_scale))\n",
    "              for k, v in keypoints.items()}\n",
    "\n",
    "        aligned = align_face_to_template(frame, kp)\n",
    "\n",
    "        if aligned is not None:\n",
    "            face_input = preprocess_face(aligned)\n",
    "\n",
    "            emb = embedding_model.predict(face_input, verbose=0)[0]\n",
    "            emb = emb / np.linalg.norm(emb)\n",
    "\n",
    "            similarity = float(\n",
    "                np.dot(emb, reference_embedding)\n",
    "            )\n",
    "\n",
    "            label = \"MATCH\" if similarity >= THRESHOLD else \"NO MATCH\"\n",
    "\n",
    "            x, y, w, h = [int(v/cached_scale) for v in box]\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{label} | sim={similarity:.3f}\",\n",
    "                (x, y-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0,255,0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "    cv2.imshow(\"ImageNet Face Verification\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
