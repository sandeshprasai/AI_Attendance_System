{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "031cb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "af79ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_FOLDER = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/DataSets/SandeshPrasai\"\n",
    "PERSON_NAME = \"Sandesh Prasai\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c47d2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def l2_norm(x, axis=1):\n",
    "    return tf.math.l2_normalize(x, axis=axis)\n",
    "\n",
    "class ArcFace(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, margin=0.5, scale=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"margin\": self.margin,\n",
    "            \"scale\": self.scale,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d11f0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_face(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not readable\")\n",
    "\n",
    "    # BGR → RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # FORCE resize (CRITICAL FIX)\n",
    "    img = cv2.resize(img, (112, 112), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Normalize (matches training)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7533de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_embeddings(folder_path, embedding_model):\n",
    "    embeddings = []\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"Extracting embeddings\"):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = load_preprocessed_face(img_path)\n",
    "            img = np.expand_dims(img, axis=0)  # (1,112,112,3)\n",
    "\n",
    "            emb = embedding_model.predict(img, verbose=0)\n",
    "            embeddings.append(emb.flatten())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIPPED] {img_name}: {e}\")\n",
    "\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1d257cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tf.keras.models.load_model(\n",
    "    r\"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/src/models/RestNet50/final_year_project_face_recognition/embedding_model.keras\",\n",
    "    custom_objects={\n",
    "        \"l2_norm\": l2_norm,\n",
    "        \"ArcFace\": ArcFace\n",
    "    },\n",
    "    compile=False   # IMPORTANT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b30f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 9/9 [00:02<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw embeddings shape: (9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "person_embeddings = extract_person_embeddings(\n",
    "    PERSON_FOLDER,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "print(\"Raw embeddings shape:\", person_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcdcc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_embeddings = normalize(person_embeddings, norm=\"l2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46af89c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embedding shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "person_embedding_mean = np.mean(person_embeddings, axis=0)\n",
    "\n",
    "print(\"Final embedding shape:\", person_embedding_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa63aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{PERSON_NAME}_embedding.npy\", person_embedding_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5bba1",
   "metadata": {},
   "source": [
    "## Match Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90507ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "285a62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference embedding shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_EMB_PATH = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/notebook/Verson_3_RestNet+ArcFace/Sandesh Prasai_embedding.npy\"\n",
    "reference_embedding = np.load(REFERENCE_EMB_PATH)\n",
    "\n",
    "print(\"Reference embedding shape:\", reference_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d560696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN(device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67fe026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_FACIAL_POINTS = np.array([\n",
    "    [38.2946, 51.6963],\n",
    "    [73.5318, 51.5014],\n",
    "    [56.0252, 71.7366],\n",
    "    [41.5493, 92.3655],\n",
    "    [70.7299, 92.2041]\n",
    "], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8083eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face_to_template(img, landmarks):\n",
    "    src_pts = np.array([\n",
    "        landmarks['left_eye'],\n",
    "        landmarks['right_eye'],\n",
    "        landmarks['nose'],\n",
    "        landmarks['mouth_left'],\n",
    "        landmarks['mouth_right']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    tform, _ = cv2.estimateAffinePartial2D(src_pts, REFERENCE_FACIAL_POINTS)\n",
    "    if tform is None:\n",
    "        return None\n",
    "\n",
    "    return cv2.warpAffine(img, tform, (112, 112), borderValue=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d467133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lighting(img):\n",
    "    yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    yuv[:, :, 0] = clahe.apply(yuv[:, :, 0])\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea622256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_face_for_embedding(face_bgr):\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = face_rgb.astype(np.float32) / 255.0\n",
    "    face_rgb = np.expand_dims(face_rgb, axis=0)  # (1,112,112,3)\n",
    "    return face_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d05483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-04 12:20:35.180585: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:20:37.496307: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:20:38.419962: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:20:40.578919: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:20:43.367126: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:20:50.203399: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n",
      "2026-02-04 12:21:24.354699: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [0,48,48,3] vs. [1,1,1,32]\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"http://192.168.1.75:4747/video\")\n",
    "\n",
    "THRESHOLD = 0.65  # start value (we can tune later)\n",
    "\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb)\n",
    "\n",
    "    if results:\n",
    "        best_face = max(results, key=lambda x: x['confidence'])\n",
    "\n",
    "        aligned = align_face_to_template(frame, best_face['keypoints'])\n",
    "\n",
    "        if aligned is not None:\n",
    "            aligned = solve_lighting(aligned)\n",
    "\n",
    "            face_input = prepare_face_for_embedding(aligned)\n",
    "\n",
    "            emb = embedding_model.predict(face_input, verbose=0)\n",
    "            emb = emb.flatten()\n",
    "            emb = emb / np.linalg.norm(emb)  # L2 normalize\n",
    "\n",
    "            similarity = cosine_similarity(\n",
    "                emb.reshape(1, -1),\n",
    "                reference_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            label = \"MATCH\" if similarity >= THRESHOLD else \"NO MATCH\"\n",
    "\n",
    "            x, y, w, h = best_face['box']\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{label} | sim={similarity:.3f}\",\n",
    "                (x, y-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0,255,0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "    cv2.imshow(\"Face Verification\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c440757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
