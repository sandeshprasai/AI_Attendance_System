{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031cb4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af79ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON_FOLDER = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/DataSets/SandeshPrasai\"\n",
    "PERSON_NAME = \"Sandesh Prasai\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c47d2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def l2_norm(x, axis=1):\n",
    "    return tf.math.l2_normalize(x, axis=axis)\n",
    "\n",
    "class ArcFace(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, margin=0.5, scale=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"margin\": self.margin,\n",
    "            \"scale\": self.scale,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11f0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_face(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        raise ValueError(\"Image not readable\")\n",
    "\n",
    "    # BGR → RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # FORCE resize (CRITICAL FIX)\n",
    "    img = cv2.resize(img, (112, 112), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Normalize (matches training)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7533de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_person_embeddings(folder_path, embedding_model):\n",
    "    embeddings = []\n",
    "\n",
    "    image_files = [\n",
    "        f for f in os.listdir(folder_path)\n",
    "        if f.lower().endswith(('.jpg', '.png', '.jpeg'))\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "    for img_name in tqdm(image_files, desc=\"Extracting embeddings\"):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "        try:\n",
    "            img = load_preprocessed_face(img_path)\n",
    "            img = np.expand_dims(img, axis=0)  # (1,112,112,3)\n",
    "\n",
    "            emb = embedding_model.predict(img, verbose=0)\n",
    "            embeddings.append(emb.flatten())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIPPED] {img_name}: {e}\")\n",
    "\n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d257cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1769749179.400738   22276 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4309 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "embedding_model = tf.keras.models.load_model(\n",
    "    r\"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/src/models/RestNet50/final_year_project_face_recognition/embedding_model.keras\",\n",
    "    custom_objects={\n",
    "        \"l2_norm\": l2_norm,\n",
    "        \"ArcFace\": ArcFace\n",
    "    },\n",
    "    compile=False   # IMPORTANT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b30f593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/9 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1769749182.583411   22541 service.cc:152] XLA service 0x7239a0015a90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1769749182.584409   22541 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2026-01-30 10:44:42.728692: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1769749183.310896   22541 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1769749185.723785   22541 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Extracting embeddings: 100%|██████████| 9/9 [00:05<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw embeddings shape: (9, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "person_embeddings = extract_person_embeddings(\n",
    "    PERSON_FOLDER,\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "print(\"Raw embeddings shape:\", person_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdcc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_embeddings = normalize(person_embeddings, norm=\"l2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46af89c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embedding shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "person_embedding_mean = np.mean(person_embeddings, axis=0)\n",
    "\n",
    "print(\"Final embedding shape:\", person_embedding_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa63aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{PERSON_NAME}_embedding.npy\", person_embedding_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af5bba1",
   "metadata": {},
   "source": [
    "## Match Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90507ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285a62df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference embedding shape: (512,)\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_EMB_PATH = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/notebook/Verson_3_RestNet+ArcFace/Sandesh Prasai_embedding.npy\"\n",
    "reference_embedding = np.load(REFERENCE_EMB_PATH)\n",
    "\n",
    "print(\"Reference embedding shape:\", reference_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d560696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN(device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67fe026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCE_FACIAL_POINTS = np.array([\n",
    "    [38.2946, 51.6963],\n",
    "    [73.5318, 51.5014],\n",
    "    [56.0252, 71.7366],\n",
    "    [41.5493, 92.3655],\n",
    "    [70.7299, 92.2041]\n",
    "], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8083eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_face_to_template(img, landmarks):\n",
    "    src_pts = np.array([\n",
    "        landmarks['left_eye'],\n",
    "        landmarks['right_eye'],\n",
    "        landmarks['nose'],\n",
    "        landmarks['mouth_left'],\n",
    "        landmarks['mouth_right']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    tform, _ = cv2.estimateAffinePartial2D(src_pts, REFERENCE_FACIAL_POINTS)\n",
    "    if tform is None:\n",
    "        return None\n",
    "\n",
    "    return cv2.warpAffine(img, tform, (112, 112), borderValue=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d467133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lighting(img):\n",
    "    yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    yuv[:, :, 0] = clahe.apply(yuv[:, :, 0])\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea622256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_face_for_embedding(face_bgr):\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = face_rgb.astype(np.float32) / 255.0\n",
    "    face_rgb = np.expand_dims(face_rgb, axis=0)  # (1,112,112,3)\n",
    "    return face_rgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d05483c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"http://192.168.1.88:4747/video\")\n",
    "\n",
    "THRESHOLD = 0.65  # start value (we can tune later)\n",
    "\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb)\n",
    "\n",
    "    if results:\n",
    "        best_face = max(results, key=lambda x: x['confidence'])\n",
    "\n",
    "        aligned = align_face_to_template(frame, best_face['keypoints'])\n",
    "\n",
    "        if aligned is not None:\n",
    "            aligned = solve_lighting(aligned)\n",
    "\n",
    "            face_input = prepare_face_for_embedding(aligned)\n",
    "\n",
    "            emb = embedding_model.predict(face_input, verbose=0)\n",
    "            emb = emb.flatten()\n",
    "            emb = emb / np.linalg.norm(emb)  # L2 normalize\n",
    "\n",
    "            similarity = cosine_similarity(\n",
    "                emb.reshape(1, -1),\n",
    "                reference_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "\n",
    "            label = \"MATCH\" if similarity >= THRESHOLD else \"NO MATCH\"\n",
    "\n",
    "            x, y, w, h = best_face['box']\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                f\"{label} | sim={similarity:.3f}\",\n",
    "                (x, y-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (0,255,0),\n",
    "                2\n",
    "            )\n",
    "\n",
    "    cv2.imshow(\"Face Verification\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c440757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 images for enrollment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from mtcnn import MTCNN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==============================\n",
    "# PATHS\n",
    "# ==============================\n",
    "PERSON_FOLDER = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/DataSets/SandeshPrasai\"\n",
    "SAVE_EMB_PATH = os.path.join(PERSON_FOLDER, \"SandeshPrasai_embedding.npy\")\n",
    "\n",
    "MODEL_PATH = \"/home/sandeshprasai/Projects/Final_Semester_Project/AI_Attendance_System/ai-ml-model/src/models/RestNet50/final_year_project_face_recognition/embedding_model.keras\"\n",
    "\n",
    "CAMERA_URL = \"http://192.168.1.81:4747/video\"\n",
    "THRESHOLD = 0.65\n",
    "\n",
    "# ==============================\n",
    "# ARC FACE HELPERS\n",
    "# ==============================\n",
    "def l2_norm(x, axis=1):\n",
    "    return tf.math.l2_normalize(x, axis=axis)\n",
    "\n",
    "class ArcFace(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes, margin=0.5, scale=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"margin\": self.margin,\n",
    "            \"scale\": self.scale,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# ==============================\n",
    "# LOAD MODEL\n",
    "# ==============================\n",
    "embedding_model = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={\"l2_norm\": l2_norm, \"ArcFace\": ArcFace},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# FACE PREPROCESSING (CANONICAL)\n",
    "# ==============================\n",
    "detector = MTCNN(device=\"cpu\")\n",
    "\n",
    "REFERENCE_FACIAL_POINTS = np.array([\n",
    "    [38.2946, 51.6963],\n",
    "    [73.5318, 51.5014],\n",
    "    [56.0252, 71.7366],\n",
    "    [41.5493, 92.3655],\n",
    "    [70.7299, 92.2041]\n",
    "], dtype=np.float32)\n",
    "\n",
    "def align_face_to_template(img, landmarks):\n",
    "    src_pts = np.array([\n",
    "        landmarks['left_eye'],\n",
    "        landmarks['right_eye'],\n",
    "        landmarks['nose'],\n",
    "        landmarks['mouth_left'],\n",
    "        landmarks['mouth_right']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    tform, _ = cv2.estimateAffinePartial2D(src_pts, REFERENCE_FACIAL_POINTS)\n",
    "    if tform is None:\n",
    "        return None\n",
    "\n",
    "    return cv2.warpAffine(img, tform, (112, 112), borderValue=0)\n",
    "\n",
    "def solve_lighting(img):\n",
    "    yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    yuv[:, :, 0] = clahe.apply(yuv[:, :, 0])\n",
    "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "def preprocess_face(img_bgr):\n",
    "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(rgb)\n",
    "\n",
    "    if not results:\n",
    "        raise ValueError(\"No face detected\")\n",
    "\n",
    "    best_face = max(results, key=lambda x: x['confidence'])\n",
    "    aligned = align_face_to_template(img_bgr, best_face['keypoints'])\n",
    "\n",
    "    if aligned is None:\n",
    "        raise ValueError(\"Alignment failed\")\n",
    "\n",
    "    aligned = solve_lighting(aligned)\n",
    "    aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
    "    aligned = aligned.astype(np.float32) / 255.0\n",
    "    aligned = np.expand_dims(aligned, axis=0)\n",
    "\n",
    "    return aligned\n",
    "\n",
    "# ==============================\n",
    "# STEP 1: ENROLLMENT (FOLDER)\n",
    "# ==============================\n",
    "embeddings = []\n",
    "\n",
    "image_files = [\n",
    "    f for f in os.listdir(PERSON_FOLDER)\n",
    "    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(image_files)} images for enrollment\")\n",
    "\n",
    "for img_name in tqdm(image_files, desc=\"Extracting embeddings\"):\n",
    "    img_path = os.path.join(PERSON_FOLDER, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        face_input = preprocess_face(img)\n",
    "        emb = embedding_model.predict(face_input, verbose=0).flatten()\n",
    "        emb = emb / np.linalg.norm(emb)\n",
    "        embeddings.append(emb)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIPPED] {img_name}: {e}\")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "reference_embedding = np.mean(embeddings, axis=0)\n",
    "reference_embedding /= np.linalg.norm(reference_embedding)\n",
    "\n",
    "np.save(SAVE_EMB_PATH, reference_embedding)\n",
    "print(\"Reference embedding saved:\", SAVE_EMB_PATH)\n",
    "\n",
    "# ==============================\n",
    "# STEP 2: LIVE CAMERA VERIFICATION\n",
    "# ==============================\n",
    "cap = cv2.VideoCapture(CAMERA_URL)\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        face_input = preprocess_face(frame)\n",
    "        emb = embedding_model.predict(face_input, verbose=0).flatten()\n",
    "        emb = emb / np.linalg.norm(emb)\n",
    "\n",
    "        similarity = cosine_similarity(\n",
    "            emb.reshape(1, -1),\n",
    "            reference_embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        label = \"MATCH\" if similarity >= THRESHOLD else \"NO MATCH\"\n",
    "        color = (0, 255, 0) if label == \"MATCH\" else (0, 0, 255)\n",
    "\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"{label} | sim={similarity:.3f}\",\n",
    "            (30, 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.9,\n",
    "            color,\n",
    "            2\n",
    "        )\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    cv2.imshow(\"Face Verification\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84790c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
